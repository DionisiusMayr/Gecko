{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51ce1c7a-518d-4807-87c3-3492c84d77ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import col, length, from_json, expr, split, lit, to_date, explode, count, lower, trim, regexp_replace\n",
    "from pyspark.sql.functions import substring, max as spark_max, ceil, input_file_name, from_unixtime, regexp_extract\n",
    "from pyspark.sql.types import StringType, StructType, StructField, MapType, ArrayType, DoubleType, DateType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c91417f-b56f-46c2-92f0-ca11a50ecda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import requests\n",
    "import os\n",
    "import collections\n",
    "import time\n",
    "import html\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import yake\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0ef305d-6533-496e-84d8-f97d3bcc8776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15ab211d-7ed4-4e16-8301-066b2549d07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "AWS_ACCESS_KEY_ID = 'test_key_id'\n",
    "AWS_SECRET_ACCESS_KEY = 'test_access_key'\n",
    "HOST = 's3'\n",
    "ENDPOINT_URL = f'http://{HOST}:4566'\n",
    "\n",
    "TEMP_DIR = './local_data'\n",
    "DOWNLOAD_FROM_S3 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3a09f6c-5799-4317-ae78-30c7152f45ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.jars.packages\", \"org.neo4j:neo4j-connector-apache-spark_2.12:5.3.0_for_spark_3\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9d6d00-b1f2-4767-a0b8-15e3fa0dbd96",
   "metadata": {},
   "source": [
    "# Collect data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "075e42ba-a87a-4d99-9719-bc4ea00a1883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTENTS = ['movie/review', 'movie/info', 'boardgame/boardgame', 'boardgame/collection', 'videogame', 'anime/user_info', 'anime/info']\n",
    "# CONTENTS = ['anime/user_info', 'anime/info']\n",
    "CONTENTS = ['movie', 'boardgame', 'videogame', 'anime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d8ad300-9823-40a2-bd4a-83f803a60af1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def download_raw_data_of_content(content):\n",
    "    print(f'Downloading raw-data of {content}...')\n",
    "    \n",
    "    target_dir = f\"{TEMP_DIR}/{content}\"\n",
    "    \n",
    "    if not os.path.exists(target_dir):\n",
    "        os.makedirs(target_dir)\n",
    "        \n",
    "    s3 = utils.S3_conn()\n",
    "\n",
    "    paginator = s3.s3_client.get_paginator('list_objects_v2')\n",
    "    page_iterator = paginator.paginate(Bucket='raw-data', Prefix=content)\n",
    "    for page in page_iterator:\n",
    "        if 'Contents' in page:\n",
    "            for obj in tqdm(page['Contents']):\n",
    "                key = obj['Key']\n",
    "                local_file_path = f'{target_dir}/{key[len(content) + 1:]}'# os.path.join(target_dir, key[len(kind):])\n",
    "                local_file_dir = os.path.dirname(local_file_path)\n",
    "                \n",
    "                if not os.path.exists(local_file_dir):\n",
    "                    os.makedirs(local_file_dir)\n",
    "                \n",
    "                s3.s3_client.download_file('raw-data', key, local_file_path)\n",
    "\n",
    "\n",
    "\n",
    "    # keys = s3.get_keys_with_prefix('raw-data', content)\n",
    "\n",
    "    \n",
    "    # for key in tqdm(keys):\n",
    "    #     local_file_path = f'{target_dir}/{key[len(content) + 1:]}'# os.path.join(target_dir, key[len(kind):])\n",
    "    #     local_file_dir = os.path.dirname(local_file_path)\n",
    "    #     if not os.path.exists(local_file_dir):\n",
    "    #         os.makedirs(local_file_dir)\n",
    "    \n",
    "    #     s3.s3_client.download_file('raw-data', key, local_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0fcf0c1-7e96-41d5-9e59-748d1e883be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading raw-data of movie...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:11<00:00, 88.44it/s]\n",
      "100%|██████████| 1000/1000 [00:10<00:00, 91.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading raw-data of boardgame...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:11<00:00, 89.72it/s]\n",
      "100%|██████████| 1000/1000 [00:11<00:00, 90.47it/s]\n",
      "100%|██████████| 1000/1000 [00:10<00:00, 91.77it/s]\n",
      "100%|██████████| 1000/1000 [00:10<00:00, 92.36it/s]\n",
      "100%|██████████| 1000/1000 [00:10<00:00, 96.82it/s]\n",
      "100%|██████████| 1000/1000 [00:10<00:00, 91.63it/s]\n",
      "100%|██████████| 1000/1000 [00:11<00:00, 88.23it/s]\n",
      "100%|██████████| 1000/1000 [00:11<00:00, 87.77it/s]\n",
      "100%|██████████| 1000/1000 [00:11<00:00, 86.53it/s]\n",
      "100%|██████████| 1000/1000 [00:11<00:00, 88.74it/s]\n",
      "100%|██████████| 1000/1000 [00:11<00:00, 90.58it/s]\n",
      "100%|██████████| 1000/1000 [00:12<00:00, 81.74it/s]\n",
      "100%|██████████| 884/884 [00:12<00:00, 73.46it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading raw-data of videogame...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:02<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading raw-data of anime...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:12<00:00, 77.10it/s]\n",
      "100%|██████████| 1000/1000 [00:12<00:00, 79.14it/s]\n",
      "100%|██████████| 1000/1000 [00:10<00:00, 91.26it/s]\n",
      "100%|██████████| 1000/1000 [00:11<00:00, 85.16it/s]\n",
      "100%|██████████| 1000/1000 [00:11<00:00, 87.28it/s]\n",
      "100%|██████████| 1000/1000 [00:10<00:00, 92.62it/s]\n",
      "100%|██████████| 1000/1000 [00:10<00:00, 91.14it/s]\n",
      "100%|██████████| 1000/1000 [00:10<00:00, 94.32it/s]\n",
      "100%|██████████| 1000/1000 [00:10<00:00, 96.43it/s]\n",
      "100%|██████████| 1000/1000 [00:10<00:00, 98.93it/s]\n",
      "100%|██████████| 1000/1000 [00:10<00:00, 98.36it/s]\n",
      "100%|██████████| 1000/1000 [00:10<00:00, 99.47it/s]\n",
      "100%|██████████| 1000/1000 [00:09<00:00, 103.04it/s]\n",
      "100%|██████████| 1000/1000 [00:10<00:00, 96.04it/s]\n",
      "100%|██████████| 1000/1000 [00:10<00:00, 98.83it/s]\n",
      "100%|██████████| 1000/1000 [00:10<00:00, 99.52it/s]\n",
      "100%|██████████| 1000/1000 [00:10<00:00, 99.15it/s]\n",
      "100%|██████████| 1000/1000 [00:10<00:00, 96.27it/s]\n",
      "100%|██████████| 1000/1000 [00:10<00:00, 97.90it/s]\n",
      "100%|██████████| 1000/1000 [00:10<00:00, 95.49it/s]\n",
      "100%|██████████| 1000/1000 [00:10<00:00, 97.62it/s]\n",
      "100%|██████████| 1000/1000 [00:10<00:00, 99.09it/s]\n",
      "100%|██████████| 1000/1000 [00:10<00:00, 96.84it/s]\n",
      "100%|██████████| 1000/1000 [00:10<00:00, 99.20it/s]\n",
      "100%|██████████| 1000/1000 [00:10<00:00, 97.17it/s]\n",
      "100%|██████████| 1000/1000 [00:09<00:00, 101.37it/s]\n",
      "100%|██████████| 1000/1000 [00:11<00:00, 88.76it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 152.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 55s, sys: 38.4 s, total: 4min 33s\n",
      "Wall time: 7min 41s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if DOWNLOAD_FROM_S3:\n",
    "    for content in CONTENTS:\n",
    "        download_raw_data_of_content(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b853d745-744a-4313-a01c-ffe7901bed4b",
   "metadata": {},
   "source": [
    "# Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "202f783a-4317-4a14-a13d-8620f8747c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = utils.S3_conn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75bea88f-cb36-4efa-9173-d423beee889e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_processed_parquet(local_directory, prefix):\n",
    "    bucket_name = 'processed-data'\n",
    "    \n",
    "    for root, dirs, files in tqdm(os.walk(local_directory)):\n",
    "        for filename in files:\n",
    "            # Construct the full local path\n",
    "            local_path = os.path.join(root, filename)\n",
    "            \n",
    "            # Construct the relative path for S3\n",
    "            relative_path = os.path.relpath(local_path, local_directory)\n",
    "            s3_path = os.path.join(prefix, relative_path).replace(\"\\\\\", \"/\")  # Ensure Unix-style paths for S3\n",
    "            \n",
    "            # Upload the file to S3\n",
    "            s3.s3_client.upload_file(local_path, bucket_name, s3_path)\n",
    "            # print(f'Uploaded {local_path} to s3://{bucket_name}/{s3_path}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad06d713-5c41-4488-b880-70f67db856d5",
   "metadata": {},
   "source": [
    "## Boardgames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdc5fcd0-6d6f-434b-bf0e-a0129a455734",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOARDGAME_USERS_XML_PATH = './local_data/boardgame/collection'\n",
    "BOARDGAME_USERS_PARQUET_PATH = './local_data/boardgame/processed_data/boardgame_users.parquet'\n",
    "BOARDGAME_CONTENT_XML_PATH = './local_data/boardgame/boardgame'\n",
    "BOARDGAME_CONTENT_PARQUET_PATH = './local_data/boardgame/processed_data/boardgame_content.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8435fc23-0e3a-4a3d-9c57-721db3a057f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xml_collection_to_dataframe(xml_file) -> pd.DataFrame:\n",
    "    with open(xml_file, 'r') as f:\n",
    "        r_text = f.read()\n",
    "        root = ET.fromstring(r_text)\n",
    "\n",
    "    df_user_id = []\n",
    "    df_type = []\n",
    "    df_content_id = []\n",
    "    df_rating = []\n",
    "    df_rating_date = []\n",
    "    \n",
    "    for bg in root:\n",
    "        bg_name = bg[0].text\n",
    "        coll_id = bg.attrib['collid']  # I don't really know what this is, but I guess it is the id of this instance of the boardgame in the list\n",
    "        object_id = bg.attrib['objectid']  # This is the boardgame identifier\n",
    "\n",
    "        rating_val = None\n",
    "        for field in bg:\n",
    "            if field.tag == 'stats':\n",
    "                rating_val = field[0].attrib['value']\n",
    "                if rating_val == 'N/A':\n",
    "                    rating_val = None\n",
    "            if field.tag == 'yearpublished':\n",
    "                year_published = field.text\n",
    "            if field.tag == 'status':\n",
    "                date_of_rating = field.attrib['lastmodified']  # Not really the rating date, but it is as close as possible with the current information.\n",
    "\n",
    "        # print(user_id, 'boardgame', object_id, rating_val, date_of_rating)\n",
    "        # print(bg_name, rating_val, year_published, coll_id, object_id)\n",
    "        df_user_id.append(xml_file.split('/')[-1][:-4])\n",
    "        df_type.append('boardgame')\n",
    "        df_content_id.append(object_id)\n",
    "        df_rating.append(rating_val)\n",
    "        df_rating_date.append(date_of_rating)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'user_id': pd.Series(df_user_id, dtype='str'),\n",
    "        'type': pd.Series(df_type, dtype='category'),\n",
    "        'content_id': pd.Series(df_content_id, dtype='str'),\n",
    "        'rating': pd.Series(df_rating, dtype='float64'),\n",
    "        'rating_date': pd.Series(df_rating_date, dtype='datetime64[ms]')\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5324700f-1612-462c-9197-d49d3e624d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_boardgame_users_parquet():\n",
    "    if not os.path.exists(BOARDGAME_USERS_PARQUET_PATH):\n",
    "        os.makedirs(BOARDGAME_USERS_PARQUET_PATH)\n",
    "        \n",
    "    for xml in filter(lambda x: x.endswith('.xml'), os.listdir(BOARDGAME_USERS_XML_PATH)):\n",
    "        try:\n",
    "            df = xml_collection_to_dataframe(f'{BOARDGAME_USERS_XML_PATH}/{xml}')\n",
    "            parquet_path = f'{BOARDGAME_USERS_PARQUET_PATH}/{xml[:-4]}.parquet'\n",
    "            df.to_parquet(parquet_path)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'Error: Invalid xml file: {xml}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a4f933a-364d-4706-9f62-623688200523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xml_boardgame_to_dataframe():\n",
    "    df_content_id = []\n",
    "    df_content_description = []\n",
    "    df_content_year = []\n",
    "\n",
    "    for folder in os.listdir(BOARDGAME_CONTENT_XML_PATH):\n",
    "        with open(f\"{BOARDGAME_CONTENT_XML_PATH}/{folder}/1.xml\", 'r') as f:\n",
    "            r_text = f.read()\n",
    "        df_content_id.append(folder)\n",
    "        root = ET.fromstring(r_text)\n",
    "        for bg in root:\n",
    "            for field in bg:\n",
    "                if field.tag == 'description':\n",
    "                    df_content_description.append(html.unescape(field.text))\n",
    "                if field.tag == 'yearpublished':\n",
    "                    df_content_year.append(int(field.attrib['value']))\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'content_id': pd.Series(df_content_id, dtype='str'),\n",
    "        'description': pd.Series(df_content_description, dtype='str'),\n",
    "        'release_year': pd.Series(df_content_year, dtype='Int16')\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b255246-7a58-4a95-9f76-5cff2887ddfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_boardgame_content_parquet():\n",
    "    if not os.path.exists(BOARDGAME_CONTENT_PARQUET_PATH):\n",
    "        os.makedirs(BOARDGAME_CONTENT_PARQUET_PATH)\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"content_id\", StringType(), True),\n",
    "        StructField(\"description\", StringType(), True),\n",
    "        StructField(\"release_year\", IntegerType(), True)\n",
    "    ])\n",
    "    \n",
    "    df = xml_boardgame_to_dataframe()\n",
    "    df['description'] = df['description'].astype('str')\n",
    "    df = df.replace([np.nan], [None])\n",
    "    \n",
    "    boardgame_content = (\n",
    "        spark\n",
    "        .createDataFrame(df, schema=schema)\n",
    "        .withColumn('type', lit('boardgame'))\n",
    "    )\n",
    "    \n",
    "    # Save parquet to processed-data zone\n",
    "    boardgame_content.write.mode('overwrite').parquet(BOARDGAME_CONTENT_PARQUET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef9f92b5-1106-4495-bc12-6849ada2478c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boardgame_users_df():\n",
    "    boardgame_users = spark.read.parquet(BOARDGAME_USERS_PARQUET_PATH)\n",
    "    return boardgame_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "812dec24-234c-4d63-8b60-8cec03fc0a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boardgame_content_df():\n",
    "    boardgame_content = spark.read.parquet(BOARDGAME_CONTENT_PARQUET_PATH)\n",
    "    return boardgame_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "480f3005-c860-4b60-a20a-0b3516dfdb1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'collid'\n",
      "Error: Invalid xml file: Century.xml\n",
      "'collid'\n",
      "Error: Invalid xml file: Icythistle.xml\n",
      "'collid'\n",
      "Error: Invalid xml file: ItsCharlieVP.xml\n",
      "'collid'\n",
      "Error: Invalid xml file: nugenet.xml\n",
      "'collid'\n",
      "Error: Invalid xml file: marioymia.xml\n",
      "'collid'\n",
      "Error: Invalid xml file: RobMcWiz.xml\n",
      "'collid'\n",
      "Error: Invalid xml file: zigooloo.xml\n",
      "'collid'\n",
      "Error: Invalid xml file: Halenor.xml\n"
     ]
    }
   ],
   "source": [
    "create_boardgame_users_parquet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b0bce9c-6e4d-4d53-b054-71f531f3d5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_boardgame_content_parquet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ded81b9-f2a3-483b-9243-086502c2e2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+----------+------+-------------------+\n",
      "|    user_id|     type|content_id|rating|        rating_date|\n",
      "+-----------+---------+----------+------+-------------------+\n",
      "|zefquaavius|boardgame|    322232|   6.0|2023-08-01 14:52:32|\n",
      "|zefquaavius|boardgame|    296402|   8.0|2023-08-02 14:18:24|\n",
      "|zefquaavius|boardgame|    336537|  null|2023-08-02 14:18:38|\n",
      "|zefquaavius|boardgame|    314445|  null|2023-08-02 14:18:54|\n",
      "|zefquaavius|boardgame|    296404|  null|2023-08-02 14:19:11|\n",
      "+-----------+---------+----------+------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- content_id: string (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- rating_date: timestamp_ntz (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "boardgame_users = get_boardgame_users_df()\n",
    "boardgame_users.show(5)\n",
    "boardgame_users.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9517615-8a64-4321-be33-963add4e875f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------+---------+\n",
      "|content_id|         description|release_year|     type|\n",
      "+----------+--------------------+------------+---------+\n",
      "|    189314|Set of seven prom...|        2015|boardgame|\n",
      "|    157661|Grifters is a han...|        2015|boardgame|\n",
      "|    174391|Exposed is a quic...|        2016|boardgame|\n",
      "|    168054|Alone is a sci-fi...|        2019|boardgame|\n",
      "|    191932|From the official...|        2012|boardgame|\n",
      "+----------+--------------------+------------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- content_id: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- release_year: integer (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "boardgame_content = get_boardgame_content_df()\n",
    "boardgame_content.show(5)\n",
    "boardgame_content.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed41e521-3d92-4ecd-9236-829d70711e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:03,  3.61s/it]\n"
     ]
    }
   ],
   "source": [
    "store_processed_parquet(BOARDGAME_USERS_PARQUET_PATH, prefix='boardgame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "108452b6-99ad-4a63-9a11-47bf84e5f39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  4.49it/s]\n"
     ]
    }
   ],
   "source": [
    "store_processed_parquet(BOARDGAME_CONTENT_PARQUET_PATH, prefix='boardgame')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0765858-e3a1-4c06-8bd1-2a00c2c29e46",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7355dce1-61e6-46a4-b035-e56f07ddc689",
   "metadata": {},
   "outputs": [],
   "source": [
    "MOVIE_BASE_PARQUET_PATH = './local_data/movie/review'\n",
    "MOVIE_BASE_INFO_PATH = './local_data/movie/info'\n",
    "MOVIE_USERS_PARQUET_PATH = \"./local_data/movie/processed_data/movie_users.parquet\"\n",
    "MOVIE_CONTENT_PARQUET_PATH = \"./local_data/movie/processed_data/movie_content.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ace4efaa-7de2-45c6-8ff6-17f5029be320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_movie_users_parquet():\n",
    "    schema = ArrayType(\n",
    "        StructType([\n",
    "            StructField(\"author\", StringType(), True),\n",
    "            StructField(\"author_details\", StructType([\n",
    "                StructField(\"rating\", StringType(), True)\n",
    "            ]), True),\n",
    "            StructField(\"created_at\", StringType(), True),\n",
    "        ])\n",
    "    )\n",
    "    \n",
    "    movie_users = spark.read.parquet(MOVIE_BASE_PARQUET_PATH)\\\n",
    "              .filter(length(\"results\")>2)\\\n",
    "              .withColumn(\"results_test\", col('results'))\\\n",
    "              .withColumn(\"results_parsed\", from_json(col(\"results_test\"), schema))\\\n",
    "              .withColumn(\"result_exploded\", explode(col(\"results_parsed\")))\\\n",
    "              .withColumn('result_exploded', col(\"result_exploded\").cast(StringType()))\n",
    "    \n",
    "    split_col = split(movie_users['result_exploded'], ', ')\n",
    "    \n",
    "    movie_users = movie_users.withColumn('author', split_col.getItem(0)) \\\n",
    "               .withColumn('author', expr(\"substring(author,2, length(author) -1)\")) \\\n",
    "               .withColumn('rating', split_col.getItem(1)) \\\n",
    "               .withColumn(\"rating\", expr(\"substring(rating, 2, length(rating) - 2)\"))\\\n",
    "               .withColumn(\"rating\", col('rating').cast(DoubleType()))\\\n",
    "               .withColumn('rating_date', split_col.getItem(2))\\\n",
    "               .withColumn('rating_date', expr(\"substring(rating_date,1, length(rating_date) -1)\"))\\\n",
    "               .withColumn(\"rating_date\", to_date(col(\"rating_date\"), \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\"))\\\n",
    "               .select(col('author').alias('user_id'), lit('movie').alias('type'), col('id').alias('content_id').cast(StringType()), 'rating', 'rating_date')\n",
    "\n",
    "    if not os.path.exists(MOVIE_USERS_PARQUET_PATH):\n",
    "        os.makedirs(MOVIE_USERS_PARQUET_PATH)\n",
    "\n",
    "    movie_users.repartition(1).write.mode('overwrite').parquet(MOVIE_USERS_PARQUET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab09e861-f7bf-4a7a-81a3-c5fdc512d3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_movie_content_parquet():\n",
    "    if not os.path.exists(MOVIE_CONTENT_PARQUET_PATH):\n",
    "        os.makedirs(MOVIE_CONTENT_PARQUET_PATH)\n",
    "    \n",
    "    movie_content = (\n",
    "        spark\n",
    "        .read.parquet(MOVIE_BASE_INFO_PATH)\n",
    "        .select(col('id').alias('content_id'), col('overview').alias('description'), col('release_date').alias('release_year'))\n",
    "        .withColumn('release_year', substring(\"release_year\", 1, 4))\n",
    "        .withColumn('type', lit('movie'))\n",
    "        .repartition(1).write.mode('overwrite').parquet(MOVIE_CONTENT_PARQUET_PATH)\n",
    "    )\n",
    "    # movie_content.repartition(1).write.mode('overwrite').parquet(MOVIE_CONTENT_PARQUET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9bcf4b4c-f553-465a-b331-17dc24bf207b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_users_df():\n",
    "    movie_users = spark.read.parquet(MOVIE_USERS_PARQUET_PATH)\n",
    "    return movie_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bea32529-7a8c-4875-aa7b-8d183af2ff18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_content_df():\n",
    "    movie_content = spark.read.parquet(MOVIE_CONTENT_PARQUET_PATH)\n",
    "    return movie_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ebf653c-5b99-44cd-ad4b-c55b49b2ab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_movie_users_parquet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "51946b04-d6ff-4bc3-91aa-b0ae779fd8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_movie_content_parquet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e85758c8-88ee-4d6e-8bb2-9b2583e86668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+----------+------+-----------+\n",
      "|           user_id| type|content_id|rating|rating_date|\n",
      "+------------------+-----+----------+------+-----------+\n",
      "|        John Chard|movie|       576|  10.0| 2017-02-10|\n",
      "|      tmdb28039023|movie|       576|   6.0| 2022-08-28|\n",
      "|Filipe Manuel Neto|movie|       576|   5.0| 2023-10-15|\n",
      "|  Manuel São Bento|movie|    850165|   7.0| 2023-12-21|\n",
      "|             r96sk|movie|    850165|   9.0| 2024-02-09|\n",
      "+------------------+-----+----------+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- content_id: string (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- rating_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movie_users = get_movie_users_df()\n",
    "movie_users.show(5)\n",
    "movie_users.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "81bcd714-6856-442b-ae5f-08d07fdcfde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------+-----+\n",
      "|content_id|         description|release_year| type|\n",
      "+----------+--------------------+------------+-----+\n",
      "|     43969|Nogreh is a young...|        2003|movie|\n",
      "|    651102|Since its first p...|        1971|movie|\n",
      "|     80957|Brian, (Luke Goss...|        2011|movie|\n",
      "|    936897|Goldy is a spirit...|        2022|movie|\n",
      "|    146536|A journey back in...|        1986|movie|\n",
      "+----------+--------------------+------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- content_id: long (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- release_year: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movie_content = get_movie_content_df()\n",
    "movie_content.show(5)\n",
    "movie_content.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ddc96c42-8f82-4fb0-b1dc-cf8a37b13bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 15.24it/s]\n"
     ]
    }
   ],
   "source": [
    "store_processed_parquet(MOVIE_USERS_PARQUET_PATH, prefix='movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b04ef0f6-7393-4708-a197-3f7fba8757b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 16.44it/s]\n"
     ]
    }
   ],
   "source": [
    "store_processed_parquet(MOVIE_CONTENT_PARQUET_PATH, prefix='movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "54575aae-54a1-4254-8523-a0b6f6d43a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GC had this code to enrich movie_users, I don't see the use yet so I am leaving this commented out.\n",
    "# movie_users = movie_users.join(movie_content, ['content_id'],'left')\n",
    "# movie_users.repartition(1).write.mode('overwrite').parquet(MOVIE_CONTENT_PARQUET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3dec0224-cb06-4811-a349-d4401744ab63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema = ArrayType(\n",
    "#     StructType([\n",
    "#         StructField(\"author\", StringType(), True),\n",
    "#         StructField(\"author_details\", StructType([\n",
    "#             StructField(\"rating\", StringType(), True)\n",
    "#         ]), True),\n",
    "#         StructField(\"created_at\", StringType(), True),\n",
    "#     ])\n",
    "# )\n",
    "\n",
    "# movie_users = spark.read.parquet(MOVIE_BASE_PARQUET_PATH)\\\n",
    "#           .filter(length(\"results\")>2)\\\n",
    "#           .withColumn(\"results_test\", col('results'))\\\n",
    "#           .withColumn(\"results_parsed\", from_json(col(\"results_test\"), schema))\\\n",
    "#           .withColumn(\"result_exploded\", explode(col(\"results_parsed\")))\\\n",
    "#           .withColumn('result_exploded', col(\"result_exploded\").cast(StringType()))\n",
    "\n",
    "# split_col = split(movie_users['result_exploded'], ', ')\n",
    "\n",
    "# movie_users = movie_users.withColumn('author', split_col.getItem(0)) \\\n",
    "#            .withColumn('author', expr(\"substring(author,2, length(author) -1)\")) \\\n",
    "#            .withColumn('rating', split_col.getItem(1)) \\\n",
    "#            .withColumn(\"rating\", expr(\"substring(rating, 2, length(rating) - 2)\"))\\\n",
    "#            .withColumn(\"rating\", col('rating').cast(DoubleType()))\\\n",
    "#            .withColumn('rating_date', split_col.getItem(2))\\\n",
    "#            .withColumn('rating_date', expr(\"substring(rating_date,1, length(rating_date) -1)\"))\\\n",
    "#            .withColumn(\"rating_date\", to_date(col(\"rating_date\"), \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\"))\\\n",
    "#            .select(col('author').alias('user_id'), lit('movie').alias('type'), col('id').alias('content_id').cast(StringType()), 'rating', 'rating_date')\n",
    "\n",
    "# movie_content = spark.read.parquet(MOVIE_BASE_INFO_PATH)\\\n",
    "#               .select(col('id').alias('content_id'),col('overview').alias('description'), col('release_date').alias('release_year'))\\\n",
    "#               .withColumn('release_year', substring(\"release_year\", 1, 4))\n",
    "\n",
    "# # movie_content.repartition(1).write.mode('overwrite').parquet(\"./parsed_data/movies_descript.parquet\")\n",
    "\n",
    "# # Re-lectura\n",
    "\n",
    "# # dfIni = spark.read.parquet(\"./parsed_data/movies_user.parquet\")\n",
    "# # dfDesc = spark.read.parquet(\"./parsed_data/movies_descript.parquet\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552ffe46-8b53-4490-abe8-d3ac5843f259",
   "metadata": {},
   "source": [
    "## Anime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c202fc57-429e-4dfe-b83e-6640ac4387a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANIME_BASE_CONTENT_PATH = './local_data/anime/info'\n",
    "ANIME_BASE_USERS_PATH = './local_data/anime/user_info'\n",
    "ANIME_TEMP_PARQUET_PATH = './local_data/anime/temp'\n",
    "ANIME_USERS_PARQUET_PATH = \"./local_data/anime/processed_data/anime_users.parquet\"\n",
    "ANIME_CONTENT_PARQUET_PATH = \"./local_data/anime/processed_data/anime_content.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "09ef7daa-6645-44f5-adc3-d6cea024d251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anime_users_parquet():\n",
    "    if not os.path.exists(ANIME_USERS_PARQUET_PATH):\n",
    "        os.makedirs(ANIME_USERS_PARQUET_PATH)\n",
    "    path_for_anime_lists = ANIME_BASE_USERS_PATH\n",
    "    user_anime_lists_paths = os.listdir(path_for_anime_lists)\n",
    "    \n",
    "    df = spark.read.json(\n",
    "        path = [f'{path_for_anime_lists}/{i}' for i in user_anime_lists_paths],\n",
    "        multiLine = True, \n",
    "        mode = 'DROPMALFORMED'\n",
    "    ).withColumn('file_name', input_file_name()).select(\n",
    "        from_unixtime(col('updated_at')).alias('rating_date'),\n",
    "        col('score').alias('rating'),\n",
    "        col('anime_id').alias('content_id'),\n",
    "        regexp_extract(col('file_name'), '\\/([^\\/]+)\\.json$', 1).alias('user_id'),\n",
    "    )\\\n",
    "    .withColumn('type', lit('anime'))\\\n",
    "    .coalesce(1).write.mode('overwrite').parquet(ANIME_USERS_PARQUET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "25a96514-e91c-4e8b-a553-aea90f892a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anime_content_parquet():\n",
    "    if not os.path.exists(ANIME_CONTENT_PARQUET_PATH):\n",
    "        os.makedirs(ANIME_CONTENT_PARQUET_PATH)\n",
    "\n",
    "    if not os.path.exists(ANIME_TEMP_PARQUET_PATH):\n",
    "        os.makedirs(ANIME_TEMP_PARQUET_PATH)\n",
    "    \n",
    "    path_for_animes = ANIME_BASE_CONTENT_PATH\n",
    "    anime_paths = os.listdir(path_for_animes)\n",
    "    \n",
    "    batch_size = 1000\n",
    "    cnt = 0\n",
    "    \n",
    "    while len(anime_paths) > cnt * batch_size :\n",
    "        df = spark.read.json(\n",
    "            path = [f'{path_for_animes}/{i}' for i in anime_paths][cnt * batch_size: (cnt + 1) * batch_size],\n",
    "            multiLine = True, \n",
    "            mode = 'DROPMALFORMED'\n",
    "        )\\\n",
    "        .dropna(subset=['data.aired.prop.from.year'])\n",
    "        df.write.mode('overwrite').parquet(f'{ANIME_TEMP_PARQUET_PATH}/{cnt}')\n",
    "        cnt += 1\n",
    "    \n",
    "    parquet_files_path = ANIME_TEMP_PARQUET_PATH\n",
    "    parquet_files = os.listdir(parquet_files_path)\n",
    "    df = spark.read.parquet(*[f'{parquet_files_path}/{i}' for i in parquet_files])\n",
    "    df.select(\n",
    "        col('data.synopsis').alias('description'),\n",
    "        col('data.title').alias('title'),\n",
    "        col('data.mal_id').cast(StringType()).alias('content_id'),\n",
    "        col('data.aired.prop.from.year').alias('release_year')\n",
    "    )\\\n",
    "    .withColumn('type', lit('anime'))\\\n",
    "    .coalesce(1).write.mode('overwrite').parquet(ANIME_CONTENT_PARQUET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "067c01a7-ea3e-425a-a685-1b34f66ad405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anime_users_df():\n",
    "    anime_users = spark.read.parquet(ANIME_USERS_PARQUET_PATH)\n",
    "    return anime_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "37ff4893-4464-420f-a3c4-cca1b9b83de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anime_content_df():\n",
    "    anime_content = spark.read.parquet(ANIME_CONTENT_PARQUET_PATH)\n",
    "    return anime_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f5699c63-31a2-48ff-8b27-547a307fc269",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_anime_users_parquet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "469e1076-4a48-42a7-a1c3-e8548d76822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_anime_content_parquet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "606ecb67-932d-4eb6-9510-d964cc71a7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+----------+---------+-----+\n",
      "|        rating_date|rating|content_id|  user_id| type|\n",
      "+-------------------+------+----------+---------+-----+\n",
      "|2023-03-17 00:35:34|     0|       918|Nabil_967|anime|\n",
      "|2023-08-13 12:15:08|    10|        21|Nabil_967|anime|\n",
      "|2022-07-05 23:35:51|     0|     48583|Nabil_967|anime|\n",
      "|2023-07-29 14:19:26|     9|     52034|Nabil_967|anime|\n",
      "|2022-03-01 19:53:09|     5|     41380|Nabil_967|anime|\n",
      "+-------------------+------+----------+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- rating_date: string (nullable = true)\n",
      " |-- rating: long (nullable = true)\n",
      " |-- content_id: long (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "anime_users = get_anime_users_df()\n",
    "anime_users.show(5)\n",
    "anime_users.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7b4bf4f5-5b60-4635-8c4a-405217a608c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+------------+-----+\n",
      "|         description|               title|content_id|release_year| type|\n",
      "+--------------------+--------------------+----------+------------+-----+\n",
      "|During their ques...|InuYasha Movie 1:...|       452|        2001|anime|\n",
      "|In the year Cosmi...|Kidou Senshi Gund...|        93|        2002|anime|\n",
      "|The final battle ...|Sword Art Online:...|     40540|        2020|anime|\n",
      "|On his way to a c...|Tensei Kizoku no ...|     52608|        2023|anime|\n",
      "|Awaking to absolu...|Sokushi Cheat ga ...|     53730|        2024|anime|\n",
      "+--------------------+--------------------+----------+------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- description: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- content_id: string (nullable = true)\n",
      " |-- release_year: long (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "anime_content = get_anime_content_df()\n",
    "anime_content.show(5)\n",
    "anime_content.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2849aa30-fa44-4684-b858-fe8f53b51a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 10.31it/s]\n"
     ]
    }
   ],
   "source": [
    "store_processed_parquet(ANIME_USERS_PARQUET_PATH, prefix='anime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fbbf06c6-d114-4845-a2a6-32aa3bc7dda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  9.11it/s]\n"
     ]
    }
   ],
   "source": [
    "store_processed_parquet(ANIME_CONTENT_PARQUET_PATH, prefix='anime')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a972e3-9f27-40a7-baac-c8745c15b9c5",
   "metadata": {},
   "source": [
    "## Videogames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "295f45c9-0a19-420b-9e3d-344c877e992d",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEOGAME_BASE_SUMMARIES_PATH = './local_data/videogame/player_profile.json'\n",
    "VIDEOGAME_BASE_PROFILES_PATH = './local_data/videogame/games_played.json'\n",
    "VIDEOGAME_BASE_GAMES_PATH = './local_data/videogame/steam_games.json'\n",
    "VIDEOGAME_USERS_PARQUET_PATH = \"./local_data/videogame/processed_data/v_users.parquet\"\n",
    "VIDEOGAME_CONTENT_PARQUET_PATH = \"./local_data/videogame/processed_data/videogame_content.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "668148b7-efe6-49e0-9351-05cd90cd10d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_videogame_users_parquet():\n",
    "    # Load player_summaries.json\n",
    "    with open(VIDEOGAME_BASE_SUMMARIES_PATH, 'r') as f:\n",
    "        player_summaries_data = json.load(f)\n",
    "    \n",
    "    # Load steam_profiles.json\n",
    "    with open(VIDEOGAME_BASE_PROFILES_PATH, 'r') as f:\n",
    "        steam_profiles_data = json.load(f)\n",
    "    \n",
    "    # Initialize list to store data\n",
    "    common_rows = []\n",
    "    \n",
    "    # Process data from steam_profiles_data\n",
    "    for steam_profiles in steam_profiles_data:\n",
    "        steamid = list(steam_profiles.keys())[0]\n",
    "        games = steam_profiles[steamid]\n",
    "        player_summary = next((summary for summary in player_summaries_data if steamid in summary), None)\n",
    "        # Check if the player summary data is available and not empty\n",
    "        if player_summary and player_summary[steamid]:\n",
    "            personaname = player_summary[steamid].get('personaname', 'Unknown')\n",
    "            for game in games:\n",
    "                appid = game['appid']\n",
    "                playtime_forever = game['playtime_forever']\n",
    "                if playtime_forever > 0:  # Skip if playtime_forever is 0\n",
    "                    common_rows.append({'user_id': personaname, 'type': 'videogame', 'content_id': appid, 'temp_rating': playtime_forever})\n",
    "    \n",
    "    # Create Spark DataFrame\n",
    "    common_df = spark.createDataFrame(common_rows)\n",
    "    \n",
    "    # Calculate max playtime_forever for each user_id\n",
    "    max_playtime = common_df.groupBy('user_id').agg(spark_max('temp_rating').alias('max_temp_rating'))\n",
    "    \n",
    "    # Join max_playtime with common_df to calculate normalized ratings\n",
    "    common_df = common_df.join(max_playtime, on='user_id')\n",
    "    common_df = common_df.withColumn('rating', (col('temp_rating') / col('max_temp_rating')) * 10)\n",
    "    \n",
    "    # Apply ceiling to the ratings\n",
    "    common_df = common_df.withColumn('rating', ceil(col('rating')))\n",
    "    \n",
    "    # Drop the 'temp_rating' and 'max_temp_rating' columns\n",
    "    common_df = common_df.drop('temp_rating', 'max_temp_rating')\n",
    "    \n",
    "    # Add a new column 'rating_date' filled with null values\n",
    "    common_df = common_df.withColumn('rating_date', lit(None).cast('string'))\n",
    "    \n",
    "    # Display the Spark DataFrame\n",
    "    # common_df.show(10)\n",
    "    \n",
    "    # Take a sample of the data. Comment or uncomment \n",
    "    # sample_df = common_df.sample(withReplacement=False, fraction=0.001)\n",
    "    # sample_df.write.parquet('sample_steam_users.parquet')\n",
    "    \n",
    "    # Save DataFrame as Parquet file\n",
    "    common_df.write.mode('overwrite').parquet(VIDEOGAME_USERS_PARQUET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "66b7b209-82bf-40cb-b2a6-d5e336012ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_videogame_content_parquet():\n",
    "    # Load the dataset from games.json\n",
    "    dataset = {}\n",
    "    if os.path.exists(VIDEOGAME_BASE_GAMES_PATH):\n",
    "        with open(VIDEOGAME_BASE_GAMES_PATH, 'r', encoding='utf-8') as fin:\n",
    "            text = fin.read()\n",
    "            if len(text) > 0:\n",
    "                dataset = json.loads(text)\n",
    "    \n",
    "    # Initialize list to store data\n",
    "    rows = []\n",
    "    \n",
    "    # Extract the relevant data\n",
    "    for app_id, game_info in dataset.items():\n",
    "        name = game_info.get('name', '')\n",
    "        release_date = game_info.get('release_date', '')\n",
    "        # Extract the year from the release_date\n",
    "        if release_date:\n",
    "            release_year = release_date.split()[-1]\n",
    "        else:\n",
    "            release_year = ''\n",
    "        description = game_info.get('detailed_description', '')\n",
    "    \n",
    "        rows.append(Row(content_id=app_id, content_title=name, release_year=release_year, description=description))\n",
    "    \n",
    "    # Create Spark DataFrame\n",
    "    df = spark.createDataFrame(rows)\n",
    "    df = df.withColumn('type', lit('videogame'))\n",
    "    \n",
    "    # Display the first few rows of the DataFrame\n",
    "    # df.show()\n",
    "    \n",
    "    # Save DataFrame as Parquet file\n",
    "    # df.write.parquet(OUTPUT_PARQUET_FILE)\n",
    "    df.write.mode('overwrite').parquet(VIDEOGAME_CONTENT_PARQUET_PATH)\n",
    "    # Stop Spark session\n",
    "    # spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "19a6ce98-04fe-42fc-b1fa-4ec8fc13d18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_videogame_users_df():\n",
    "    videogame_users = spark.read.parquet(VIDEOGAME_USERS_PARQUET_PATH)\n",
    "    return videogame_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9712b582-1652-4722-8701-5984d90479af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_videogame_content_df():\n",
    "    videogame_content = spark.read.parquet(VIDEOGAME_CONTENT_PARQUET_PATH)\n",
    "    return videogame_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "038c56d7-e375-4acc-b707-014bbcc593dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_videogame_users_parquet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "67b67d64-1c9d-4fe9-895d-2c05e2829dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_videogame_content_parquet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "01ed97db-dcef-41e5-b239-9f10bc4488e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+------+-----------+\n",
      "|user_id|content_id|     type|rating|rating_date|\n",
      "+-------+----------+---------+------+-----------+\n",
      "|   Fooo|       300|videogame|     1|       null|\n",
      "|   Fooo|      4000|videogame|     1|       null|\n",
      "|   Fooo|      2600|videogame|     1|       null|\n",
      "|   Fooo|       220|videogame|     1|       null|\n",
      "|   Fooo|       500|videogame|     1|       null|\n",
      "+-------+----------+---------+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- content_id: long (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- rating: long (nullable = true)\n",
      " |-- rating_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "videogame_users = get_videogame_users_df()\n",
    "videogame_users.show(5)\n",
    "videogame_users.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "29b5823e-8c42-4339-b4f6-75770a185aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------+--------------------+---------+\n",
      "|content_id|       content_title|release_year|         description|     type|\n",
      "+----------+--------------------+------------+--------------------+---------+\n",
      "|    837390|        My zero trip|        2018|My zero trip is a...|videogame|\n",
      "|   1564580|Nevertales: Faryo...|        2021|Mad Head Games re...|videogame|\n",
      "|    263560|      Paper Sorcerer|        2014|Paper Sorcerer is...|videogame|\n",
      "|    831230|    Doors Quest Demo|        2018|Doors Quest Demo ...|videogame|\n",
      "|   1738500|       velvet clouds|        2021|Velvet Clouds - a...|videogame|\n",
      "+----------+--------------------+------------+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- content_id: string (nullable = true)\n",
      " |-- content_title: string (nullable = true)\n",
      " |-- release_year: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "videogame_content = get_videogame_content_df()\n",
    "videogame_content.show(5)\n",
    "videogame_content.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "40d5fe6f-1d9e-40fd-923e-da760c5dfa77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 13.48it/s]\n"
     ]
    }
   ],
   "source": [
    "store_processed_parquet(VIDEOGAME_USERS_PARQUET_PATH, prefix='videogame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "37193680-a69f-480d-bbec-4bbe485797a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.38it/s]\n"
     ]
    }
   ],
   "source": [
    "store_processed_parquet(VIDEOGAME_CONTENT_PARQUET_PATH, prefix='videogame')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac3ee52-5f72-4413-95fd-9900ce409979",
   "metadata": {},
   "source": [
    "# Merging all content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6d123d9e-db41-4014-93c7-5c35596ed28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- content_id: string (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- rating_date: string (nullable = true)\n",
      "\n",
      "+-----------+---------+----------+------+-------------------+\n",
      "|    user_id|     type|content_id|rating|        rating_date|\n",
      "+-----------+---------+----------+------+-------------------+\n",
      "|zefquaavius|boardgame|    322232|   6.0|2023-08-01 14:52:32|\n",
      "|zefquaavius|boardgame|    296402|   8.0|2023-08-02 14:18:24|\n",
      "|zefquaavius|boardgame|    336537|  null|2023-08-02 14:18:38|\n",
      "|zefquaavius|boardgame|    314445|  null|2023-08-02 14:18:54|\n",
      "|zefquaavius|boardgame|    296404|  null|2023-08-02 14:19:11|\n",
      "|zefquaavius|boardgame|    296406|  null|2023-08-01 14:56:42|\n",
      "|zefquaavius|boardgame|    322429|  null|2023-08-02 14:19:32|\n",
      "|zefquaavius|boardgame|    309782|   6.0|2023-08-01 14:57:01|\n",
      "|zefquaavius|boardgame|    314446|  null|2023-08-02 14:19:57|\n",
      "|zefquaavius|boardgame|    296407|  null|2023-08-01 14:57:31|\n",
      "|zefquaavius|boardgame|    296423|  null|2023-08-01 14:57:46|\n",
      "|zefquaavius|boardgame|    296411|  null|2023-08-02 14:20:11|\n",
      "|zefquaavius|boardgame|    321710|  null|2023-08-02 14:20:23|\n",
      "|zefquaavius|boardgame|    309530|   4.0|2023-08-01 14:58:03|\n",
      "|zefquaavius|boardgame|    314444|  null|2023-08-02 14:20:37|\n",
      "|zefquaavius|boardgame|    336618|   7.0|2023-08-01 14:58:12|\n",
      "|zefquaavius|boardgame|    296272|  null|2023-08-02 14:21:09|\n",
      "|zefquaavius|boardgame|    309523|   7.0|2023-08-02 14:21:22|\n",
      "|zefquaavius|boardgame|    309658|   6.0|2023-08-01 15:00:31|\n",
      "|zefquaavius|boardgame|    324093|   7.0|2023-08-01 15:00:39|\n",
      "+-----------+---------+----------+------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merged_users = (\n",
    "    boardgame_users\n",
    "    .union(movie_users.select(['user_id', 'type', 'content_id', 'rating', 'rating_date']))\n",
    "    .union(anime_users.select(['user_id', 'type', 'content_id', 'rating', 'rating_date']))\n",
    "    .union(videogame_users.select(['user_id', 'type', 'content_id', 'rating', 'rating_date']))\n",
    "    .withColumn('user_id', trim(lower(col('user_id'))))\n",
    "    .withColumn('type', lower(col('type')))\n",
    ")\n",
    "\n",
    "merged_users.printSchema()\n",
    "merged_users.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "16d13174-9c5b-4944-886d-790b8826fbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------+---------+\n",
      "|content_id|         description|release_year|     type|\n",
      "+----------+--------------------+------------+---------+\n",
      "|    189314|Set of seven prom...|        2015|boardgame|\n",
      "|    157661|Grifters is a han...|        2015|boardgame|\n",
      "|    174391|Exposed is a quic...|        2016|boardgame|\n",
      "|    168054|Alone is a sci-fi...|        2019|boardgame|\n",
      "|    191932|From the official...|        2012|boardgame|\n",
      "|    181495|The theme of the ...|        2015|boardgame|\n",
      "|    131581|A quick trivia ga...|        2014|boardgame|\n",
      "|    210179|Description from ...|        2016|boardgame|\n",
      "|    168314|One Night Ultimat...|        2015|boardgame|\n",
      "|    209320|Promotional card ...|        2017|boardgame|\n",
      "|    163166|From the publishe...|        2015|boardgame|\n",
      "|    214319|From the publishe...|        2005|boardgame|\n",
      "|    181613|Dead Man's Draw i...|        2015|boardgame|\n",
      "|     16620|Jinx is a new Gam...|        2012|boardgame|\n",
      "|    149155|This is a promoti...|        2016|boardgame|\n",
      "|    119407|A child's first d...|        1957|boardgame|\n",
      "|    199738|Ether Wars is a s...|        2017|boardgame|\n",
      "|     11737|BAM!: Extrahart –...|        2015|boardgame|\n",
      "|    172278|Amidst the frozen...|        2015|boardgame|\n",
      "|    181906|Description from ...|        2015|boardgame|\n",
      "+----------+--------------------+------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "boardgame_content.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "edf9c434-de31-402b-a785-42c03dca87c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+------------+-----+\n",
      "|         description|               title|content_id|release_year| type|\n",
      "+--------------------+--------------------+----------+------------+-----+\n",
      "|During their ques...|InuYasha Movie 1:...|       452|        2001|anime|\n",
      "|In the year Cosmi...|Kidou Senshi Gund...|        93|        2002|anime|\n",
      "|The final battle ...|Sword Art Online:...|     40540|        2020|anime|\n",
      "|On his way to a c...|Tensei Kizoku no ...|     52608|        2023|anime|\n",
      "|Awaking to absolu...|Sokushi Cheat ga ...|     53730|        2024|anime|\n",
      "|Eager to know why...|Dorei-ku The Anim...|     36525|        2018|anime|\n",
      "|Unleashing a deva...|Naruto: Shippuude...|      4437|        2008|anime|\n",
      "|Running into your...|Shinmai Maou no T...|     23233|        2015|anime|\n",
      "|There exist few h...|Rakudai Kishi no ...|     30296|        2015|anime|\n",
      "|In the wake of Er...|Shingeki no Kyoji...|     51535|        2023|anime|\n",
      "|Yukiteru Amano is...|    Mirai Nikki (TV)|     10620|        2011|anime|\n",
      "|Kousei Arima is a...|Shigatsu wa Kimi ...|     23273|        2014|anime|\n",
      "|Ginga Eiyuu Dense...|Ginga Eiyuu Dense...|      3665|        1999|anime|\n",
      "|With his parents ...|Monster Musume no...|     30307|        2015|anime|\n",
      "|Long before Rikuo...|Nurarihyon no Mag...|     10049|        2011|anime|\n",
      "|Every day, a youn...|    Fukumenkei Noise|     33203|        2017|anime|\n",
      "|On December 24, 2...|        Guilty Crown|     10793|        2011|anime|\n",
      "|Living an abnorma...| Uchuu Patrol Luluco|     32681|        2016|anime|\n",
      "|Right as he is ab...|Seija Musou: Sala...|     53263|        2023|anime|\n",
      "|Academy City stan...|Toaru Kagaku no A...|     38480|        2019|anime|\n",
      "+--------------------+--------------------+----------+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "anime_content.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bb696fb3-8bbc-4d85-be1d-a6b900529517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------+---------+\n",
      "|content_id|         description|release_year|     type|\n",
      "+----------+--------------------+------------+---------+\n",
      "|    189314|Set of seven prom...|        2015|boardgame|\n",
      "|    157661|Grifters is a han...|        2015|boardgame|\n",
      "|    174391|Exposed is a quic...|        2016|boardgame|\n",
      "|    168054|Alone is a sci-fi...|        2019|boardgame|\n",
      "|    191932|From the official...|        2012|boardgame|\n",
      "|    181495|The theme of the ...|        2015|boardgame|\n",
      "|    131581|A quick trivia ga...|        2014|boardgame|\n",
      "|    210179|Description from ...|        2016|boardgame|\n",
      "|    168314|One Night Ultimat...|        2015|boardgame|\n",
      "|    209320|Promotional card ...|        2017|boardgame|\n",
      "|    163166|From the publishe...|        2015|boardgame|\n",
      "|    214319|From the publishe...|        2005|boardgame|\n",
      "|    181613|Dead Man's Draw i...|        2015|boardgame|\n",
      "|     16620|Jinx is a new Gam...|        2012|boardgame|\n",
      "|    149155|This is a promoti...|        2016|boardgame|\n",
      "|    119407|A child's first d...|        1957|boardgame|\n",
      "|    199738|Ether Wars is a s...|        2017|boardgame|\n",
      "|     11737|BAM!: Extrahart –...|        2015|boardgame|\n",
      "|    172278|Amidst the frozen...|        2015|boardgame|\n",
      "|    181906|Description from ...|        2015|boardgame|\n",
      "+----------+--------------------+------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merged_content = (\n",
    "    boardgame_content\n",
    "    .union(movie_content.select(['content_id', 'description', 'release_year', 'type']))\n",
    "    .union(anime_content.select(['content_id', 'description', 'release_year', 'type']))\n",
    "    .union(videogame_content.select(['content_id', 'description', 'release_year', 'type']))\n",
    "    # .withColumn('user_id', trim(lower(col('user_id'))))\n",
    "    # .withColumn('type', lower(col('type')))\n",
    ")\n",
    "\n",
    "merged_content.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b9a3acb8-9578-4a41-b684-ff0b3dde23b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|     type| count|\n",
      "+---------+------+\n",
      "|    anime|388227|\n",
      "|videogame|289674|\n",
      "|boardgame|258496|\n",
      "|    movie|    36|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    merged_users\n",
    "    .select('type')\n",
    "    .groupBy(col('type'))\n",
    "    .count()\n",
    "    .sort(col('count'), ascending=False)\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "aff487b1-213b-4716-a9d6-9c2361345e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|     type|count|\n",
      "+---------+-----+\n",
      "|videogame|85103|\n",
      "|    anime|25039|\n",
      "|boardgame|11226|\n",
      "|    movie| 1000|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    merged_content\n",
    "    .select('type')\n",
    "    .groupBy(col('type'))\n",
    "    .count()\n",
    "    .sort(col('count'), ascending=False)\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1359205c-5e90-4a31-a8be-b5de37be295e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mstop\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6841792-7754-4eed-b50b-96c6acb1e694",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_users.groupBy(col('user_id')).count().sort(col('count'), ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19086a1-8b81-45dc-9e50-4543478a86fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    merged_users\n",
    "    .select('user_id', 'type')\n",
    "    .withColumn('user_id', trim(regexp_replace(lower(col('user_id')), '[^a-zA-Z0-9]', '')))\n",
    "    .distinct()\n",
    "    .groupBy(col('user_id'))\n",
    "    .count()\n",
    "    .sort(col('count'), ascending=False)\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca9acef-45c6-4711-bda9-660e6f00f0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    merged_users\n",
    "    .filter(merged_users['user_id'] == 'daimyo')\n",
    "    .groupBy(col('type'))\n",
    "    .count()\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553f0ff2-4c0d-41b5-a381-cc7e165381b6",
   "metadata": {},
   "source": [
    "# Yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cae6e0-5616-4c0f-aabb-a6725e7898da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5d4989-ee54-4b39-bc8e-8f6f51cacdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boardgame_content.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7728930-2bcb-499d-a752-21174903c844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = boardgame_content.rdd.map(lambda x: (x[2], get_kw(x[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f94161d-da44-4340-ae15-9c7ef8c6c2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.createDataFrame(r).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ee219f-bcb8-4495-a503-38d5d633d97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kw(text):\n",
    "   kw_extractor = yake.KeywordExtractor(\n",
    "       lan='en',\n",
    "       n=2,  # Max n-gram size\n",
    "       top=5  # Number of keywords\n",
    "   )\n",
    "    \n",
    "   return list(map(lambda x: str.lower(x[0]) if x else '', kw_extractor.extract_keywords(text)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2154aff3-370d-465d-a510-2e77a1bcd147",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = boardgame_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a43655-f4a2-41ae-85f5-a6bbe6c847e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rddK = df.rdd.map(lambda x: (x['content_id'], get_kw(x['description'])))\n",
    "rddK = spark.createDataFrame(rddK).select(col('_1').alias('content_id'), col('_2').alias('keyword'))\n",
    "dfK = (rddK.withColumn(\"keyword_1\", expr(\"keyword[0]\"))\n",
    "                .withColumn(\"keyword_2\", expr(\"keyword[1]\"))\n",
    "                .withColumn(\"keyword_3\", expr(\"keyword[2]\"))\n",
    "                .withColumn(\"keyword_4\", expr(\"keyword[3]\"))\n",
    "                .withColumn(\"keyword_5\", expr(\"keyword[4]\"))\n",
    "                .select('content_id','keyword_1','keyword_2','keyword_3','keyword_4','keyword_5' )\n",
    "      )\n",
    "dfK.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bc79eb-3f43-4eee-8c60-fa6ea8432978",
   "metadata": {},
   "source": [
    "- [ ] connect directly spark to neo4j (using the right connector)\n",
    "- [ ] maybe provide some analytics about the users' profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331bf011-4f76-4b1c-9980-bea108dcea84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what the RS does for the NULL values.\n",
    "# - We could impute something, like the average score the user gives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce826eeb-01c7-48f7-8f05-beef72d44f23",
   "metadata": {},
   "source": [
    "# Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00605030-10bc-47fa-9c53-6106c96f8d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEO4J_URL = 'bolt://neo4j:7687'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28924f47-01bc-48fd-a2aa-7bfc3808745e",
   "metadata": {},
   "outputs": [],
   "source": [
    "type = 'Boardgame'\n",
    "(\n",
    "    merged_content\n",
    "    .filter(merged_content['type'] == str.lower(type))\n",
    "    .write\n",
    "    .format(\"org.neo4j.spark.DataSource\")\n",
    "    .mode(\"Append\")\n",
    "    .option(\"labels\", f\":{type}\")\n",
    "    .option(\"url\", NEO4J_URL)\n",
    "    .save()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bff002b-aee7-4861-baba-5bd244df68b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d692155-e59d-4e15-875c-e0b850299042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = (\n",
    "#     SparkSession\n",
    "#     .builder\n",
    "#     .appName(\"Neo4j-Spark Connector\")\n",
    "#     # .config(\"spark.jars.packages\", \"neo4j-contrib:neo4j-spark-connector:5.3.0_for_spark_3\")\n",
    "#     .config(\"spark.jars.packages\", \"org.neo4j:neo4j-connector-apache-spark_2.12:5.3.0_for_spark_3\")\n",
    "#     # $SPARK_HOME/bin/pyspark --packages org.neo4j:neo4j-connector-apache-spark_2.12:5.3.0_for_spark_3\n",
    "#     .config(\"spark.neo4j.bolt.url\", NEO4J_URL)\n",
    "#     # .config(\"spark.neo4j.bolt.url\", \"bolt://neo4j\")\n",
    "#     # .config(\"spark.neo4j.bolt.user\", \"neo4j\")\n",
    "#     # .config(\"spark.neo4j.bolt.password\", \"password\")\n",
    "#     .getOrCreate()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc237fae-853e-482c-8b6c-9415bc6900b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a61b62f-9708-4b84-999f-576064680cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_df = spark.createDataFrame([\n",
    "    Row(id=1, name=\"Alice\"),\n",
    "    Row(id=2, name=\"Bob\")\n",
    "])\n",
    "\n",
    "# relationships_df = spark.createDataFrame([\n",
    "#     Row(src=1, dst=2, relationship=\"KNOWS\")\n",
    "# ])\n",
    "\n",
    "# Write nodes to Neo4j\n",
    "(\n",
    "    nodes_df\n",
    "    .write\n",
    "    .format(\"org.neo4j.spark.DataSource\")\n",
    "    # .mode(\"Overwrite\")\n",
    "    .mode(\"Append\")\n",
    "    .option(\"labels\", \":Person\")\n",
    "    .option(\"url\", NEO4J_URL)\n",
    "    .save()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe21528-5dd6-4e96-9e2b-221f4e19b79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0696b97-b3da-4a19-a081-128228db1611",
   "metadata": {},
   "outputs": [],
   "source": [
    "DRIVER = neo4j.GraphDatabase.driver(uri=\"neo4j://neo4j\")\n",
    "\n",
    "\n",
    "def execute(query: str):\n",
    "    \"\"\"\n",
    "    Executes a Cypher @query and returns its result.\n",
    "    \"\"\"\n",
    "    result = DRIVER.execute_query(query)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef24fd9-c588-4af7-8b0e-3c26599d02f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "execute('MATCH (n1)-[r]->(n2) RETURN r, n1, n2 LIMIT 25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc36d059-6126-4c3c-864d-9b5c2e77388e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "import neo4j\n",
    "import yake\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "SEED = 13\n",
    "# SEMANTIC_PATH = '../semanticscholar_raw_data'\n",
    "SEMANTIC_PATH = '../small_sample'\n",
    "DEFAULT_JOURNAL_NAME = 'Unknown'\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DRIVER = neo4j.GraphDatabase.driver(uri=\"neo4j://localhost\")\n",
    "\n",
    "\n",
    "def execute(query: str):\n",
    "    \"\"\"\n",
    "    Executes a Cypher @query and returns its result.\n",
    "    \"\"\"\n",
    "    result = DRIVER.execute_query(query)\n",
    "    return result\n",
    "\n",
    "\n",
    "def delete_graph() -> None:\n",
    "    \"\"\"\n",
    "    Deletes every node and edge of the graph.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "        MATCH (n)\n",
    "        DETACH DELETE n;\n",
    "    \"\"\"\n",
    "\n",
    "    execute(query)\n",
    "\n",
    "\n",
    "def parse_journal_name(paper) -> str:\n",
    "    \"\"\"\n",
    "    Not every file has a field 'journal' in the json.\n",
    "    This function treats those edge cases.\n",
    "    \"\"\"\n",
    "    if 'journal' not in paper or not paper['journal']:\n",
    "        return DEFAULT_JOURNAL_NAME\n",
    "    else:\n",
    "        return paper.get('journal', {'name': DEFAULT_JOURNAL_NAME}).get('name', DEFAULT_JOURNAL_NAME).replace(\"'\", '').replace('\"', '')\n",
    "\n",
    "\n",
    "def sanitize_abstract(abstract: str) -> str:\n",
    "    if abstract:\n",
    "        return (\n",
    "            abstract\n",
    "            .replace('\"', \"'\")\n",
    "            .replace('\\\\', '\\\\\\\\')\n",
    "        )\n",
    "    else:\n",
    "        return abstract\n",
    "\n",
    "\n",
    "def create_papers():\n",
    "    \"\"\"\n",
    "    Create the nodes of label `Paper`.\n",
    "    \"\"\"\n",
    "    # This is used to extract the keywords from the abstract.\n",
    "    kw_extractor = yake.KeywordExtractor(\n",
    "        lan='en',\n",
    "        n=3,  # Max n-gram size\n",
    "        top=5  # Number of keywords\n",
    "    )\n",
    "\n",
    "    for fname in tqdm(os.listdir(SEMANTIC_PATH)):\n",
    "        with open(f'{SEMANTIC_PATH}/{fname}') as f:\n",
    "            paper = json.loads(f.read())\n",
    "\n",
    "        title = paper['title'].replace('\\\\', '').replace('\"', \"'\")\n",
    "        keywords = kw_extractor.extract_keywords(paper['abstract']) if paper['abstract'] else ''\n",
    "        keywords = list(map(lambda x: str.lower(x[0]) if x else '', keywords))\n",
    "\n",
    "        # publication_venue: \"{paper['publicationVenue']}\",\n",
    "        # venue: \"{paper['venue']}\",\n",
    "        # fieldsOfStudy: {paper['fieldsOfStudy'] if paper['fieldsOfStudy'] else '[]'},\n",
    "        query = f\"\"\"\n",
    "        CREATE (n:Paper {{\n",
    "            paper_id: \"{paper['paperId']}\",\n",
    "            title: \"{title}\",\n",
    "            year: toInteger({paper['year'] if paper['year'] else -1}),\n",
    "            publicationDate: date(\"{paper['publicationDate'] if paper['publicationDate'] else '1970-01-01'}\"),\n",
    "            abstract: \"{sanitize_abstract(paper['abstract'])}\",\n",
    "            keywords: {keywords}\n",
    "        }})\n",
    "        \"\"\"\n",
    "        try:\n",
    "            execute(query)\n",
    "        except:\n",
    "            print(query)\n",
    "\n",
    "\n",
    "def create_paper__paper_id__range_index():\n",
    "    \"\"\"\n",
    "    Create indexes\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "        CREATE RANGE INDEX paper__paper_id__range_index IF NOT EXISTS\n",
    "        FOR (n:Paper)\n",
    "        ON (n.paper_id)\n",
    "    \"\"\"\n",
    "\n",
    "    execute(query)\n",
    "\n",
    "\n",
    "def create_author__author_id__range_index():\n",
    "    \"\"\"\n",
    "    Create indexes\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "        CREATE RANGE INDEX author__author_id__range_index IF NOT EXISTS\n",
    "        FOR (n:Author)\n",
    "        ON (n.author_id)\n",
    "    \"\"\"\n",
    "\n",
    "    execute(query)\n",
    "\n",
    "\n",
    "def create_authors() -> None:\n",
    "    \"\"\"\n",
    "    For each paper, generate a node with label `Author` for that paper.\n",
    "    We are using the MERGE here since we don't want to duplicate authors.\n",
    "    \"\"\"\n",
    "    for fname in tqdm(os.listdir(SEMANTIC_PATH)):\n",
    "        # print(f'Creating the authors of {fname}')\n",
    "\n",
    "        with open(f'{SEMANTIC_PATH}/{fname}') as f:\n",
    "            paper = json.loads(f.read())\n",
    "\n",
    "        for author in paper['authors']:\n",
    "            query = f\"\"\"\n",
    "            MERGE (n:Author {{\n",
    "                name: \"{author['name']}\",\n",
    "                author_id: \"{author['authorId']}\"\n",
    "            }})\n",
    "            \"\"\"\n",
    "            execute(query)\n",
    "\n",
    "\n",
    "def link_author_to_paper() -> None:\n",
    "    \"\"\"\n",
    "    Create the edge `Wrote` and `IsCorrespondingAuthor`, linking Authors and Papers.\n",
    "    The first author is considered the corresponding author.\n",
    "    \"\"\"\n",
    "    for fname in tqdm(os.listdir(SEMANTIC_PATH)):\n",
    "        # print(f'Linking authors of file {fname}')\n",
    "\n",
    "        with open(f'{SEMANTIC_PATH}/{fname}') as f:\n",
    "            paper = json.loads(f.read())\n",
    "\n",
    "            is_first = True\n",
    "            for author in paper['authors']:\n",
    "                if is_first:\n",
    "                    # The first author is the main corresponding author.\n",
    "                    query = f\"\"\"\n",
    "                        MATCH (a:Author {{author_id: '{author['authorId']}'}})\n",
    "                        WITH a\n",
    "                        MATCH (p:Paper {{paper_id: '{paper['paperId']}'}})\n",
    "                        WITH a, p\n",
    "                        CREATE (a)-[e:IsCorrespondingAuthor]->(p);\n",
    "                    \"\"\"\n",
    "                    execute(query)\n",
    "                    is_first = False\n",
    "\n",
    "                query = f\"\"\"\n",
    "                    MATCH (a:Author {{author_id: '{author['authorId']}'}})\n",
    "                    WITH a\n",
    "                    MATCH (p:Paper {{paper_id: '{paper['paperId']}'}})\n",
    "                    WITH a, p\n",
    "                    CREATE (a)-[e:Wrote]->(p);\n",
    "                \"\"\"\n",
    "\n",
    "                execute(query)\n",
    "\n",
    "\n",
    "def link_citations_between_papers() -> None:\n",
    "    \"\"\"\n",
    "    Generate the edge Cited linking a Paper to a Paper.\n",
    "    \"\"\"\n",
    "    for fname in tqdm(os.listdir(SEMANTIC_PATH)):\n",
    "        # print(f'Linking citations of file {fname}')\n",
    "\n",
    "        with open(f'{SEMANTIC_PATH}/{fname}') as f:\n",
    "            paper = json.loads(f.read())\n",
    "\n",
    "        for citation in paper.get('citations', []):\n",
    "            query = f\"\"\"\n",
    "                MATCH (a:Paper {{paper_id: '{citation['paperId']}'}})\n",
    "                WITH a\n",
    "                MATCH (p:Paper {{paper_id: '{paper['paperId']}'}})\n",
    "                CREATE (a)-[e:Cites]->(p);\n",
    "            \"\"\"\n",
    "            execute(query)\n",
    "\n",
    "\n",
    "def create_journals() -> None:\n",
    "    \"\"\"\n",
    "    Create the Journal nodes.\n",
    "    \"\"\"\n",
    "    for fname in tqdm(os.listdir(SEMANTIC_PATH)):\n",
    "        with open(f'{SEMANTIC_PATH}/{fname}') as f:\n",
    "            paper = json.loads(f.read())\n",
    "\n",
    "        journal_name = parse_journal_name(paper)\n",
    "\n",
    "        if journal_name != DEFAULT_JOURNAL_NAME:\n",
    "            query = f\"\"\"\n",
    "                MERGE (n:Journal {{\n",
    "                    year: toInteger({paper['year'] if paper['year'] else -1}),\n",
    "                    name: \"{journal_name}\"\n",
    "                }})\n",
    "            \"\"\"\n",
    "\n",
    "            execute(query)\n",
    "\n",
    "\n",
    "def link_journals()-> None:\n",
    "    \"\"\"\n",
    "    Link a Paper to a Journal creating the `PublishedIn` edge.\n",
    "    \"\"\"\n",
    "    for fname in tqdm(os.listdir(SEMANTIC_PATH)):\n",
    "        with open(f'{SEMANTIC_PATH}/{fname}') as f:\n",
    "            paper = json.loads(f.read())\n",
    "\n",
    "        query = f\"\"\"\n",
    "            MATCH (p:Paper {{paper_id: '{paper['paperId']}'}})\n",
    "            WITH p\n",
    "            MATCH (j:Journal {{name: '{parse_journal_name(paper)}', year: toInteger({paper['year'] if paper['year'] else -1})}})\n",
    "            WITH p, j\n",
    "            CREATE (p)-[e:PublishedIn]->(j);\n",
    "        \"\"\"\n",
    "        execute(query)\n",
    "\n",
    "\n",
    "def change_to_conference() -> None:\n",
    "    \"\"\"\n",
    "    Change the label from Journal to Conference if the \"Journal\" name contains 'conference' in it.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "        MATCH (j:Journal)\n",
    "        WHERE toLower(j.name) =~ '.*conference.*'\n",
    "           OR toLower(j.name) =~ '.*workshop.*'\n",
    "           OR toLower(j.name) =~ '.*proc\\..*'\n",
    "        REMOVE j:Journal\n",
    "        SET j:ConfWork\n",
    "    \"\"\"\n",
    "\n",
    "    execute(query)\n",
    "\n",
    "\n",
    "def get_possible_reviewers():\n",
    "    \"\"\"\n",
    "    Auxiliary function that returns an aggregation of all possible reviewers of a paper.\n",
    "    The logic of a \"possible reviewer\" is to select an author who:\n",
    "    1. wrote paper(s) cited by the paper in question; and who\n",
    "    2. didn't wrote the paper itself.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "        MATCH (a:Author)-[w1:Wrote]->(mp:Paper)-[c:Cites]->(cp:Paper)\n",
    "        WITH mp, cp, a\n",
    "        MATCH (wcp:Author)-[w2:Wrote]->(cp)\n",
    "        WHERE NOT (wcp)-[:Wrote]->(mp)\n",
    "        RETURN mp.paper_id AS paper_id, collect(wcp.author_id) AS possible_reviewer_ids;\n",
    "    \"\"\"\n",
    "\n",
    "    return execute(query)\n",
    "\n",
    "\n",
    "def link_reviewer_to_paper() -> None:\n",
    "    \"\"\"\n",
    "    This function generates synthetic data.\n",
    "    \"\"\"\n",
    "    result = get_possible_reviewers()\n",
    "\n",
    "    for paper_id, possible_reviewers in tqdm(result[0]):\n",
    "        # Papers can have a different amount of reviewers, varying from 1 to 4, following the distribution specified by `p`.\n",
    "        # Edge case: If the paper doesn't cite any other paper, it will have 0 reviewers.\n",
    "        reviewer_qty = min(\n",
    "            np.random.choice(np.arange(1, 5), p=[0.1, 0.3, 0.5, 0.1]),\n",
    "            len(possible_reviewers)\n",
    "        )\n",
    "\n",
    "        reviewers = random.sample(possible_reviewers, reviewer_qty)\n",
    "        for reviewer in reviewers:\n",
    "            query = f\"\"\"\n",
    "                MATCH (a:Author {{author_id: '{reviewer}'}})\n",
    "                WITH a\n",
    "                MATCH (p:Paper {{paper_id: '{paper_id}'}})\n",
    "                CREATE (a)-[e:Reviewed]->(p);\n",
    "            \"\"\"\n",
    "\n",
    "            execute(query)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"Deleting graph\")\n",
    "    delete_graph()\n",
    "    print(\"Papers\")\n",
    "    create_papers()\n",
    "    create_paper__paper_id__range_index()\n",
    "    print(\"Authors\")\n",
    "    create_author__author_id__range_index()\n",
    "    create_authors()\n",
    "    print(\"Wrote\")\n",
    "    link_author_to_paper()\n",
    "    print(\"Citations\")\n",
    "    link_citations_between_papers()\n",
    "    print(\"Journals/Conferences\")\n",
    "    create_journals()\n",
    "    print(\"Linking journals\")\n",
    "    link_journals()\n",
    "    change_to_conference()\n",
    "    print(\"Reviewers\")\n",
    "    link_reviewer_to_paper()\n",
    "\n",
    "\n",
    "    print(\"Querying...\")\n",
    "\n",
    "    print(\"Query 1\")\n",
    "    execute(\"\"\"\n",
    "MATCH (p:Paper)-[:cited]->(cited:Paper) WITH p.name AS journal, p.title AS title, COUNT(*) AS num_citations ORDER BY journal, num_citations DESC WITH journal, COLLECT({title: title, num_citations: num_citations}) AS papers WITH journal, papers, [i IN RANGE(1, SIZE(papers)) | i] AS ranks UNWIND ranks AS rank WITH journal, papers[rank - 1].title AS title, papers[rank - 1].num_citations AS num_citations, rank WHERE rank <= 3 RETURN journal, title, num_citations, rank ORDER BY journal, rank\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"Query 2\")\n",
    "    execute(\"\"\"\n",
    "MATCH (a:Author)-[:Wrote]->(p:Paper)-[:PublishedIn]->(c:ConfWork)\n",
    "WITH a.name AS author, collect(DISTINCT c.year) AS years, c.name AS conference\n",
    "WHERE size(years) > 3\n",
    "RETURN author, years, conference\n",
    "ORDER BY author, conference\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"Query 3\")\n",
    "    execute(\"\"\"\n",
    "MATCH (citing_paper:Paper)-[:Cites]->(published_paper:Paper {year: j.year})-[:PublishedIn]->(j:Journal)\n",
    "WITH COUNT(DISTINCT citing_paper) AS total_citations, j.name AS journal_name, j AS j1\n",
    "MATCH (j2: Journal)<-[:PublishedIn]-(p:Paper)\n",
    "WHERE j2.year IN [j1.year - 1, j1.year - 2]\n",
    "      AND j1.name = j2.name\n",
    "WITH j1.year AS year,\n",
    "     COUNT(p.title) AS past_publications,\n",
    "     j1.name AS journal_name,\n",
    "     total_citations\n",
    "RETURN year, journal_name, total_citations, past_publications, 1.0 * total_citations / past_publications\n",
    "ORDER BY journal_name, year;\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"Query 4\")\n",
    "    execute(\"\"\"\n",
    "MATCH (a:Author)-[:Wrote]->(p:Paper)-[:cited]->(cited:Paper) WITH a, p, COUNT(*) AS num_citations ORDER BY num_citations DESC WITH a, COLLECT(num_citations) AS citation_counts WITH a, [i IN RANGE(1, SIZE(citation_counts)) | CASE WHEN citation_counts[i - 1] >= i THEN i ELSE 0 END] AS h_values WITH a, MAX(h_values) AS h_index WITH a, MAX(REDUCE(s = 0, h IN h_index | CASE WHEN h > s THEN h ELSE s END)) AS max_h_index RETURN a.author_id AS author_id, a.name AS author_name, max_h_index\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"Recommendation system\")\n",
    "    print(\"Part 1\")\n",
    "    execute(\"\"\"\n",
    "// First we are looking for papers containing any of those keywords.\n",
    "MATCH (p:Paper)\n",
    "WHERE\n",
    "    // Could've been an array intersection, but APOC was giving us some setup issues.\n",
    "    'data management' IN p.keywords\n",
    "    OR 'indexing' IN p.keywords\n",
    "    OR 'data modeling' IN p.keywords\n",
    "    OR 'big data' IN p.keywords\n",
    "    OR 'data processing' IN p.keywords\n",
    "    OR 'data storage' IN p.keywords\n",
    "    OR 'data querying' IN p.keywords\n",
    "RETURN *\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"Part 2\")\n",
    "    execute(\"\"\"\n",
    "// Now we want the conferences or journals with at least 90% of published papers being related to databases.\n",
    "MATCH (p:Paper)-[:PublishedIn]->(jc)\n",
    "WITH p, (\n",
    "        'data management' IN p.keywords\n",
    "        OR 'indexing' IN p.keywords\n",
    "        OR 'data modeling' IN p.keywords\n",
    "        OR 'big data' IN p.keywords\n",
    "        OR 'data processing' IN p.keywords\n",
    "        OR 'data storage' IN p.keywords\n",
    "        OR 'data querying' IN p.keywords\n",
    "    ) AS in_db_community,\n",
    "    jc\n",
    "WITH COUNT(p) AS total_published_papers, SUM(CASE in_db_community WHEN TRUE THEN 1 ELSE 0 END) AS db_comm_papers, jc.name AS jc_name\n",
    "WHERE 100.0 * db_comm_papers / total_published_papers > 90.0 \n",
    "RETURN total_published_papers, db_comm_papers, 100.0 * db_comm_papers / total_published_papers AS percentage_of_db_papers, jc_name\n",
    "LIMIT 50\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"Part 3\")\n",
    "    execute(\"\"\"\n",
    "// Let's now grab the top 100 most cited papers in the Database community.\n",
    "MATCH (p:Paper)-[:PublishedIn]->(jc)\n",
    "WITH p, (\n",
    "        'data management' IN p.keywords\n",
    "        OR 'indexing' IN p.keywords\n",
    "        OR 'data modeling' IN p.keywords\n",
    "        OR 'big data' IN p.keywords\n",
    "        OR 'data processing' IN p.keywords\n",
    "        OR 'data storage' IN p.keywords\n",
    "        OR 'data querying' IN p.keywords\n",
    "    ) AS in_db_community,\n",
    "    jc\n",
    "WITH COUNT(p) AS total_published_papers, SUM(CASE in_db_community WHEN TRUE THEN 1 ELSE 0 END) AS db_comm_papers, jc.name AS jc_name, jc\n",
    "WHERE 100.0 * db_comm_papers / total_published_papers > 90.0\n",
    "WITH collect(jc.name) AS db_comm_conferences\n",
    "\n",
    "MATCH (citing_paper:Paper)-[:Cites]->(cited_paper:Paper)-[:PublishedIn]->(jc1), (citing_paper)-[:PublishedIn]->(jc2)\n",
    "WHERE jc1.name IN db_comm_conferences\n",
    "  AND jc2.name IN db_comm_conferences\n",
    "WITH cited_paper, jc1, COUNT(DISTINCT citing_paper) AS c\n",
    "RETURN c, jc1.name, cited_paper.title\n",
    "ORDER BY c DESC\n",
    "LIMIT 100\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"Part 4\")\n",
    "    execute(\"\"\"\n",
    "// Now, we will find the gurus of the community.\n",
    "MATCH (p:Paper)-[:PublishedIn]->(jc)\n",
    "WITH p, (\n",
    "        'data management' IN p.keywords\n",
    "        OR 'indexing' IN p.keywords\n",
    "        OR 'data modeling' IN p.keywords\n",
    "        OR 'big data' IN p.keywords\n",
    "        OR 'data processing' IN p.keywords\n",
    "        OR 'data storage' IN p.keywords\n",
    "        OR 'data querying' IN p.keywords\n",
    "    ) AS in_db_community,\n",
    "    jc\n",
    "WITH COUNT(p) AS total_published_papers, SUM(CASE in_db_community WHEN TRUE THEN 1 ELSE 0 END) AS db_comm_papers, jc.name AS jc_name, jc\n",
    "WHERE 100.0 * db_comm_papers / total_published_papers > 90.0\n",
    "WITH collect(jc.name) AS db_comm_conferences\n",
    "\n",
    "MATCH (citing_paper:Paper)-[:Cites]->(cited_paper:Paper)-[:PublishedIn]->(jc1), (citing_paper)-[:PublishedIn]->(jc2)\n",
    "WHERE jc1.name IN db_comm_conferences\n",
    "  AND jc2.name IN db_comm_conferences\n",
    "WITH cited_paper, jc1, COUNT(DISTINCT citing_paper) AS c\n",
    "WITH COLLECT(cited_paper.paper_id)[1..100] AS most_cited_papers// UNWIND most_cited_papers AS most_cited_paper\n",
    "\n",
    "MATCH (p1:Paper)<-[:Wrote]-(a:Author)-[:Wrote]->(p2:Paper)\n",
    "WHERE p1 <> p2\n",
    "AND p1.paper_id IN most_cited_papers\n",
    "AND p2.paper_id IN most_cited_papers\n",
    "RETURN a\n",
    "LIMIT 100\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "    print(\"Graph Algorithms\")\n",
    "    print(\"Article Rank\")\n",
    "    execute(\"\"\"\n",
    "CALL gds.graph.drop('part_d_1', FALSE);\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "    execute(\"\"\"\n",
    "CALL gds.graph.project(\n",
    "  'part_d_1',\n",
    "  'Paper',\n",
    "  ['Cites', 'PublishedIn']\n",
    ");\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "    execute(\"\"\"\n",
    "CALL gds.articleRank.stream('part_d_1')\n",
    "YIELD nodeId, score\n",
    "RETURN gds.util.asNode(nodeId).title AS name, score\n",
    "ORDER BY score DESC, name ASC;\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"Node Similarity\")\n",
    "    execute(\"\"\"\n",
    "CALL gds.graph.drop('part_d_2', FALSE);\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "    execute(\"\"\"\n",
    "CALL gds.graph.project(\n",
    "  'part_d_2',\n",
    "  ['Author', 'Paper'],\n",
    "  ['Wrote', 'IsCorrespondingAuthor']\n",
    ");\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "    execute(\"\"\"\n",
    "CALL gds.nodeSimilarity.stream('part_d_2')\n",
    "YIELD node1, node2, similarity\n",
    "RETURN gds.util.asNode(node1).name AS Author_1, gds.util.asNode(node2).name AS Author_2, similarity\n",
    "ORDER BY similarity DESC, Author_1, Author_2\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "    print(\"Evolving the graph examples\")\n",
    "    execute(\"\"\"\n",
    "// Adding an affiliation\n",
    "MATCH (a:Author {author_id: '2174735571'})\n",
    "CREATE (a)-[:Affiliated]->(:University {name: 'UPC'});\n",
    "    \"\"\")\n",
    "\n",
    "    execute(\"\"\"\n",
    "// Adding a new review\n",
    "MATCH (a:Author {author_id: '2174735571'}) WITH a\n",
    "MATCH (p:Paper {paper_id: '1da6ce9007a17c60697ca563419d7cc7949ab639'})\n",
    "CREATE (a)-[:Reviewed {review_text: 'Some comments about xyz...', accepted: TRUE}]->(p);\n",
    "    \"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

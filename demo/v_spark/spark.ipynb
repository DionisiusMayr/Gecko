{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51ce1c7a-518d-4807-87c3-3492c84d77ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import col, length, from_json, expr, split, lit, to_date, explode, count, lower, trim, regexp_replace\n",
    "from pyspark.sql.functions import substring, max as spark_max, ceil, input_file_name, from_unixtime, regexp_extract, concat\n",
    "from pyspark.sql.types import StringType, StructType, StructField, MapType, ArrayType, DoubleType, DateType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c91417f-b56f-46c2-92f0-ca11a50ecda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import requests\n",
    "import os\n",
    "import collections\n",
    "import time\n",
    "import html\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import yake\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0ef305d-6533-496e-84d8-f97d3bcc8776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15ab211d-7ed4-4e16-8301-066b2549d07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "AWS_ACCESS_KEY_ID = 'test_key_id'\n",
    "AWS_SECRET_ACCESS_KEY = 'test_access_key'\n",
    "HOST = 's3'\n",
    "ENDPOINT_URL = f'http://{HOST}:4566'\n",
    "\n",
    "TEMP_DIR = './local_data'\n",
    "DOWNLOAD_FROM_S3 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3a09f6c-5799-4317-ae78-30c7152f45ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.jars.packages\", \"org.neo4j:neo4j-connector-apache-spark_2.12:5.3.0_for_spark_3\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9d6d00-b1f2-4767-a0b8-15e3fa0dbd96",
   "metadata": {},
   "source": [
    "# Collect data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "075e42ba-a87a-4d99-9719-bc4ea00a1883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTENTS = ['movie/review', 'movie/info', 'boardgame/boardgame', 'boardgame/collection', 'videogame', 'anime/user_info', 'anime/info']\n",
    "# CONTENTS = ['anime/user_info', 'anime/info']\n",
    "CONTENTS = ['movie', 'boardgame', 'videogame', 'anime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d8ad300-9823-40a2-bd4a-83f803a60af1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def download_raw_data_of_content(content):\n",
    "    print(f'Downloading raw-data of {content}...')\n",
    "    \n",
    "    target_dir = f\"{TEMP_DIR}/{content}\"\n",
    "    \n",
    "    if not os.path.exists(target_dir):\n",
    "        os.makedirs(target_dir)\n",
    "        \n",
    "    s3 = utils.S3_conn()\n",
    "\n",
    "    paginator = s3.s3_client.get_paginator('list_objects_v2')\n",
    "    page_iterator = paginator.paginate(Bucket='raw-data', Prefix=content)\n",
    "    for page in page_iterator:\n",
    "        if 'Contents' in page:\n",
    "            for obj in tqdm(page['Contents']):\n",
    "                key = obj['Key']\n",
    "                local_file_path = f'{target_dir}/{key[len(content) + 1:]}'# os.path.join(target_dir, key[len(kind):])\n",
    "                local_file_dir = os.path.dirname(local_file_path)\n",
    "                \n",
    "                if not os.path.exists(local_file_dir):\n",
    "                    os.makedirs(local_file_dir)\n",
    "                \n",
    "                s3.s3_client.download_file('raw-data', key, local_file_path)\n",
    "\n",
    "\n",
    "\n",
    "    # keys = s3.get_keys_with_prefix('raw-data', content)\n",
    "\n",
    "    \n",
    "    # for key in tqdm(keys):\n",
    "    #     local_file_path = f'{target_dir}/{key[len(content) + 1:]}'# os.path.join(target_dir, key[len(kind):])\n",
    "    #     local_file_dir = os.path.dirname(local_file_path)\n",
    "    #     if not os.path.exists(local_file_dir):\n",
    "    #         os.makedirs(local_file_dir)\n",
    "    \n",
    "    #     s3.s3_client.download_file('raw-data', key, local_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0fcf0c1-7e96-41d5-9e59-748d1e883be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 3 µs, total: 5 µs\n",
      "Wall time: 9.3 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if DOWNLOAD_FROM_S3:\n",
    "    for content in CONTENTS:\n",
    "        download_raw_data_of_content(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b853d745-744a-4313-a01c-ffe7901bed4b",
   "metadata": {},
   "source": [
    "# Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "202f783a-4317-4a14-a13d-8620f8747c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = utils.S3_conn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75bea88f-cb36-4efa-9173-d423beee889e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_processed_parquet(local_directory, prefix):\n",
    "    bucket_name = 'processed-data'\n",
    "    \n",
    "    for root, dirs, files in tqdm(os.walk(local_directory)):\n",
    "        for filename in files:\n",
    "            # Construct the full local path\n",
    "            local_path = os.path.join(root, filename)\n",
    "            \n",
    "            # Construct the relative path for S3\n",
    "            relative_path = os.path.relpath(local_path, local_directory)\n",
    "            s3_path = os.path.join(prefix, relative_path).replace(\"\\\\\", \"/\")  # Ensure Unix-style paths for S3\n",
    "            \n",
    "            # Upload the file to S3\n",
    "            s3.s3_client.upload_file(local_path, bucket_name, s3_path)\n",
    "            # print(f'Uploaded {local_path} to s3://{bucket_name}/{s3_path}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad06d713-5c41-4488-b880-70f67db856d5",
   "metadata": {},
   "source": [
    "## Boardgames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdc5fcd0-6d6f-434b-bf0e-a0129a455734",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOARDGAME_USERS_XML_PATH = './local_data/boardgame/collection'\n",
    "BOARDGAME_USERS_PARQUET_PATH = './local_data/boardgame/processed_data/boardgame_users.parquet'\n",
    "BOARDGAME_CONTENT_XML_PATH = './local_data/boardgame/boardgame'\n",
    "BOARDGAME_CONTENT_PARQUET_PATH = './local_data/boardgame/processed_data/boardgame_content.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8435fc23-0e3a-4a3d-9c57-721db3a057f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xml_collection_to_dataframe(xml_file) -> pd.DataFrame:\n",
    "    with open(xml_file, 'r') as f:\n",
    "        r_text = f.read()\n",
    "        root = ET.fromstring(r_text)\n",
    "\n",
    "    df_user_id = []\n",
    "    df_type = []\n",
    "    df_content_id = []\n",
    "    df_rating = []\n",
    "    df_rating_date = []\n",
    "    \n",
    "    for bg in root:\n",
    "        bg_name = bg[0].text\n",
    "        coll_id = bg.attrib['collid']  # I don't really know what this is, but I guess it is the id of this instance of the boardgame in the list\n",
    "        object_id = bg.attrib['objectid']  # This is the boardgame identifier\n",
    "\n",
    "        rating_val = None\n",
    "        for field in bg:\n",
    "            if field.tag == 'stats':\n",
    "                rating_val = field[0].attrib['value']\n",
    "                if rating_val == 'N/A':\n",
    "                    rating_val = None\n",
    "            if field.tag == 'yearpublished':\n",
    "                year_published = field.text\n",
    "            if field.tag == 'status':\n",
    "                date_of_rating = field.attrib['lastmodified']  # Not really the rating date, but it is as close as possible with the current information.\n",
    "\n",
    "        # print(user_id, 'boardgame', object_id, rating_val, date_of_rating)\n",
    "        # print(bg_name, rating_val, year_published, coll_id, object_id)\n",
    "        df_user_id.append(xml_file.split('/')[-1][:-4])\n",
    "        df_type.append('boardgame')\n",
    "        df_content_id.append(object_id)\n",
    "        df_rating.append(rating_val)\n",
    "        df_rating_date.append(date_of_rating)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'user_id': pd.Series(df_user_id, dtype='str'),\n",
    "        'type': pd.Series(df_type, dtype='category'),\n",
    "        'content_id': pd.Series(df_content_id, dtype='str'),\n",
    "        'rating': pd.Series(df_rating, dtype='float64'),\n",
    "        'rating_date': pd.Series(df_rating_date, dtype='datetime64[ms]')\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5324700f-1612-462c-9197-d49d3e624d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_boardgame_users_parquet():\n",
    "    if not os.path.exists(BOARDGAME_USERS_PARQUET_PATH):\n",
    "        os.makedirs(BOARDGAME_USERS_PARQUET_PATH)\n",
    "        \n",
    "    for xml in filter(lambda x: x.endswith('.xml'), os.listdir(BOARDGAME_USERS_XML_PATH)):\n",
    "        try:\n",
    "            df = xml_collection_to_dataframe(f'{BOARDGAME_USERS_XML_PATH}/{xml}')\n",
    "            parquet_path = f'{BOARDGAME_USERS_PARQUET_PATH}/{xml[:-4]}.parquet'\n",
    "            df.to_parquet(parquet_path)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'Error: Invalid xml file: {xml}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a4f933a-364d-4706-9f62-623688200523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xml_boardgame_to_dataframe():\n",
    "    df_content_id = []\n",
    "    df_content_description = []\n",
    "    df_content_year = []\n",
    "    df_title = []\n",
    "\n",
    "    for folder in os.listdir(BOARDGAME_CONTENT_XML_PATH):\n",
    "        with open(f\"{BOARDGAME_CONTENT_XML_PATH}/{folder}/1.xml\", 'r') as f:\n",
    "            r_text = f.read()\n",
    "        df_content_id.append(folder)\n",
    "        root = ET.fromstring(r_text)\n",
    "        for bg in root:\n",
    "            for field in bg:\n",
    "                if field.tag == 'name' and field.attrib['type'] == 'primary':\n",
    "                    df_title.append(field.attrib['value'])\n",
    "                if field.tag == 'description':\n",
    "                    df_content_description.append(html.unescape(field.text))\n",
    "                if field.tag == 'yearpublished':\n",
    "                    df_content_year.append(int(field.attrib['value']))\n",
    "    return pd.DataFrame({\n",
    "        'content_id': pd.Series(df_content_id, dtype='str'),\n",
    "        'description': pd.Series(df_content_description, dtype='str'),\n",
    "        'release_year': pd.Series(df_content_year, dtype='Int16'),\n",
    "        'title': pd.Series(df_title, dtype='str')\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b255246-7a58-4a95-9f76-5cff2887ddfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_boardgame_content_parquet():\n",
    "    if not os.path.exists(BOARDGAME_CONTENT_PARQUET_PATH):\n",
    "        os.makedirs(BOARDGAME_CONTENT_PARQUET_PATH)\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"content_id\", StringType(), True),\n",
    "        StructField(\"description\", StringType(), True),\n",
    "        StructField(\"release_year\", IntegerType(), True),\n",
    "        StructField(\"title\", StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    df = xml_boardgame_to_dataframe()\n",
    "    df['description'] = df['description'].astype('str')\n",
    "    df = df.replace([np.nan], [None])\n",
    "    \n",
    "    boardgame_content = (\n",
    "        spark\n",
    "        .createDataFrame(df, schema=schema)\n",
    "        .withColumn('type', lit('boardgame'))\n",
    "    )\n",
    "    \n",
    "    # Save parquet to processed-data zone\n",
    "    boardgame_content.write.mode('overwrite').parquet(BOARDGAME_CONTENT_PARQUET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef9f92b5-1106-4495-bc12-6849ada2478c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boardgame_users_df():\n",
    "    boardgame_users = spark.read.parquet(BOARDGAME_USERS_PARQUET_PATH)\n",
    "    return boardgame_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "812dec24-234c-4d63-8b60-8cec03fc0a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boardgame_content_df():\n",
    "    boardgame_content = spark.read.parquet(BOARDGAME_CONTENT_PARQUET_PATH)\n",
    "    return boardgame_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "480f3005-c860-4b60-a20a-0b3516dfdb1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'collid'\n",
      "Error: Invalid xml file: Icythistle.xml\n",
      "'collid'\n",
      "Error: Invalid xml file: Century.xml\n",
      "'collid'\n",
      "Error: Invalid xml file: zigooloo.xml\n",
      "'collid'\n",
      "Error: Invalid xml file: RobMcWiz.xml\n",
      "'collid'\n",
      "Error: Invalid xml file: nugenet.xml\n",
      "'collid'\n",
      "Error: Invalid xml file: ItsCharlieVP.xml\n",
      "'collid'\n",
      "Error: Invalid xml file: Halenor.xml\n",
      "'collid'\n",
      "Error: Invalid xml file: marioymia.xml\n"
     ]
    }
   ],
   "source": [
    "create_boardgame_users_parquet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b0bce9c-6e4d-4d53-b054-71f531f3d5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_boardgame_content_parquet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ded81b9-f2a3-483b-9243-086502c2e2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+----------+------+-------------------+\n",
      "|    user_id|     type|content_id|rating|        rating_date|\n",
      "+-----------+---------+----------+------+-------------------+\n",
      "|zefquaavius|boardgame|    322232|   6.0|2023-08-01 14:52:32|\n",
      "|zefquaavius|boardgame|    296402|   8.0|2023-08-02 14:18:24|\n",
      "|zefquaavius|boardgame|    336537|  null|2023-08-02 14:18:38|\n",
      "|zefquaavius|boardgame|    314445|  null|2023-08-02 14:18:54|\n",
      "|zefquaavius|boardgame|    296404|  null|2023-08-02 14:19:11|\n",
      "+-----------+---------+----------+------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- content_id: string (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- rating_date: timestamp_ntz (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "boardgame_users = get_boardgame_users_df()\n",
    "boardgame_users.show(5)\n",
    "boardgame_users.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9517615-8a64-4321-be33-963add4e875f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------+--------------------+---------+\n",
      "|content_id|         description|release_year|               title|     type|\n",
      "+----------+--------------------+------------+--------------------+---------+\n",
      "|     13191|From the publishe...|        1980|  Super Quintillions|boardgame|\n",
      "|      2120|A very simple gam...|        2000|Looney Tunes Trad...|boardgame|\n",
      "|     13005|Dance of Ibexes, ...|        2004|     Dance of Ibexes|boardgame|\n",
      "|      1425|This solitaire wa...|        1987| Raid on St. Nazaire|boardgame|\n",
      "|      1096|Party trivia game...|        1999|           Four Real|boardgame|\n",
      "+----------+--------------------+------------+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- content_id: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- release_year: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "boardgame_content = get_boardgame_content_df()\n",
    "boardgame_content.show(5)\n",
    "boardgame_content.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed41e521-3d92-4ecd-9236-829d70711e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_processed_parquet(BOARDGAME_USERS_PARQUET_PATH, prefix='boardgame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "108452b6-99ad-4a63-9a11-47bf84e5f39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_processed_parquet(BOARDGAME_CONTENT_PARQUET_PATH, prefix='boardgame')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0765858-e3a1-4c06-8bd1-2a00c2c29e46",
   "metadata": {},
   "source": [
    "## Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7355dce1-61e6-46a4-b035-e56f07ddc689",
   "metadata": {},
   "outputs": [],
   "source": [
    "MOVIE_BASE_PARQUET_PATH = './local_data/movie/review'\n",
    "MOVIE_BASE_INFO_PATH = './local_data/movie/info'\n",
    "MOVIE_USERS_PARQUET_PATH = \"./local_data/movie/processed_data/movie_users.parquet\"\n",
    "MOVIE_CONTENT_PARQUET_PATH = \"./local_data/movie/processed_data/movie_content.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ace4efaa-7de2-45c6-8ff6-17f5029be320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_movie_users_parquet():\n",
    "    schema = ArrayType(\n",
    "        StructType([\n",
    "            StructField(\"author\", StringType(), True),\n",
    "            StructField(\"author_details\", StructType([\n",
    "                StructField(\"rating\", StringType(), True)\n",
    "            ]), True),\n",
    "            StructField(\"created_at\", StringType(), True),\n",
    "        ])\n",
    "    )\n",
    "    \n",
    "    movie_users = spark.read.parquet(MOVIE_BASE_PARQUET_PATH)\\\n",
    "              .filter(length(\"results\")>2)\\\n",
    "              .withColumn(\"results_test\", col('results'))\\\n",
    "              .withColumn(\"results_parsed\", from_json(col(\"results_test\"), schema))\\\n",
    "              .withColumn(\"result_exploded\", explode(col(\"results_parsed\")))\\\n",
    "              .withColumn('result_exploded', col(\"result_exploded\").cast(StringType()))\n",
    "    \n",
    "    split_col = split(movie_users['result_exploded'], ', ')\n",
    "    \n",
    "    movie_users = movie_users.withColumn('author', split_col.getItem(0)) \\\n",
    "               .withColumn('author', expr(\"substring(author,2, length(author) -1)\")) \\\n",
    "               .withColumn('rating', split_col.getItem(1)) \\\n",
    "               .withColumn(\"rating\", expr(\"substring(rating, 2, length(rating) - 2)\"))\\\n",
    "               .withColumn(\"rating\", col('rating').cast(DoubleType()))\\\n",
    "               .withColumn('rating_date', split_col.getItem(2))\\\n",
    "               .withColumn('rating_date', expr(\"substring(rating_date,1, length(rating_date) -1)\"))\\\n",
    "               .withColumn(\"rating_date\", to_date(col(\"rating_date\"), \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\"))\\\n",
    "               .select(col('author').alias('user_id'), lit('movie').alias('type'), col('id').alias('content_id').cast(StringType()), 'rating', 'rating_date')\n",
    "\n",
    "    if not os.path.exists(MOVIE_USERS_PARQUET_PATH):\n",
    "        os.makedirs(MOVIE_USERS_PARQUET_PATH)\n",
    "\n",
    "    movie_users.repartition(1).write.mode('overwrite').parquet(MOVIE_USERS_PARQUET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab09e861-f7bf-4a7a-81a3-c5fdc512d3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_movie_content_parquet():\n",
    "    if not os.path.exists(MOVIE_CONTENT_PARQUET_PATH):\n",
    "        os.makedirs(MOVIE_CONTENT_PARQUET_PATH)\n",
    "    \n",
    "    movie_content = (\n",
    "        spark\n",
    "        .read.parquet(MOVIE_BASE_INFO_PATH)\n",
    "        .select(col('id').alias('content_id'), col('overview').alias('description'), col('release_date').alias('release_year'),col('original_title').alias('title'))\n",
    "        .withColumn('release_year', substring(\"release_year\", 1, 4))\n",
    "        .withColumn('type', lit('movie'))\n",
    "        .repartition(1).write.mode('overwrite').parquet(MOVIE_CONTENT_PARQUET_PATH)\n",
    "    )\n",
    "    # movie_content.repartition(1).write.mode('overwrite').parquet(MOVIE_CONTENT_PARQUET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9bcf4b4c-f553-465a-b331-17dc24bf207b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_users_df():\n",
    "    movie_users = spark.read.parquet(MOVIE_USERS_PARQUET_PATH)\n",
    "    return movie_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bea32529-7a8c-4875-aa7b-8d183af2ff18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_content_df():\n",
    "    movie_content = spark.read.parquet(MOVIE_CONTENT_PARQUET_PATH)\n",
    "    return movie_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ebf653c-5b99-44cd-ad4b-c55b49b2ab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_movie_users_parquet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51946b04-d6ff-4bc3-91aa-b0ae779fd8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_movie_content_parquet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e85758c8-88ee-4d6e-8bb2-9b2583e86668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+----------+------+-----------+\n",
      "|           user_id| type|content_id|rating|rating_date|\n",
      "+------------------+-----+----------+------+-----------+\n",
      "|        John Chard|movie|       576|  10.0| 2017-02-10|\n",
      "|      tmdb28039023|movie|       576|   6.0| 2022-08-28|\n",
      "|Filipe Manuel Neto|movie|       576|   5.0| 2023-10-15|\n",
      "|  Manuel São Bento|movie|    850165|   7.0| 2023-12-21|\n",
      "|             r96sk|movie|    850165|   9.0| 2024-02-09|\n",
      "+------------------+-----+----------+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- content_id: string (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- rating_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movie_users = get_movie_users_df()\n",
    "movie_users.show(5)\n",
    "movie_users.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81bcd714-6856-442b-ae5f-08d07fdcfde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------+--------------------+-----+\n",
      "|content_id|         description|release_year|               title| type|\n",
      "+----------+--------------------+------------+--------------------+-----+\n",
      "|     43969|Nogreh is a young...|        2003|             پنج عصر|movie|\n",
      "|    651102|Since its first p...|        1971|       Chân Trời Tím|movie|\n",
      "|     80957|Brian, (Luke Goss...|        2011|             Pressed|movie|\n",
      "|    936897|Goldy is a spirit...|        2022|Curious Caterer: ...|movie|\n",
      "|    146536|A journey back in...|        1986|Os Trapalhões no ...|movie|\n",
      "+----------+--------------------+------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- content_id: long (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- release_year: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movie_content = get_movie_content_df()\n",
    "movie_content.show(5)\n",
    "movie_content.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ddc96c42-8f82-4fb0-b1dc-cf8a37b13bb1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "S3UploadFailedError",
     "evalue": "Failed to upload ./local_data/movie/processed_data/movie_users.parquet/.part-00000-221c4992-6444-4e02-8fc4-ce406aa3a911-c000.snappy.parquet.crc to processed-data/movie/.part-00000-221c4992-6444-4e02-8fc4-ce406aa3a911-c000.snappy.parquet.crc: An error occurred (NoSuchBucket) when calling the PutObject operation: The specified bucket does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchBucket\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/boto3/s3/transfer.py:371\u001b[0m, in \u001b[0;36mS3Transfer.upload_file\u001b[0;34m(self, filename, bucket, key, callback, extra_args)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 371\u001b[0m     \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;66;03m# If a client error was raised, add the backwards compatibility layer\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;66;03m# that raises a S3UploadFailedError. These specific errors were only\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;66;03m# ever thrown for upload_parts but now can be thrown for any related\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;66;03m# client error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/s3transfer/futures.py:103\u001b[0m, in \u001b[0;36mTransferFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# Usually the result() method blocks until the transfer is done,\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# however if a KeyboardInterrupt is raised we want want to exit\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;66;03m# out of this and propagate the exception.\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_coordinator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/s3transfer/futures.py:266\u001b[0m, in \u001b[0;36mTransferCoordinator.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m--> 266\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/s3transfer/tasks.py:139\u001b[0m, in \u001b[0;36mTask.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_coordinator\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_main\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/s3transfer/tasks.py:162\u001b[0m, in \u001b[0;36mTask._execute_main\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecuting task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with kwargs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs_to_display\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 162\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_main\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# If the task is the final task, then set the TransferFuture's\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# value to the return value from main().\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/s3transfer/upload.py:764\u001b[0m, in \u001b[0;36mPutObjectTask._main\u001b[0;34m(self, client, fileobj, bucket, key, extra_args)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fileobj \u001b[38;5;28;01mas\u001b[39;00m body:\n\u001b[0;32m--> 764\u001b[0m     \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mput_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBucket\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbucket\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/client.py:565\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 565\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/client.py:1021\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mNoSuchBucket\u001b[0m: An error occurred (NoSuchBucket) when calling the PutObject operation: The specified bucket does not exist",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mS3UploadFailedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mstore_processed_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMOVIE_USERS_PARQUET_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmovie\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 14\u001b[0m, in \u001b[0;36mstore_processed_parquet\u001b[0;34m(local_directory, prefix)\u001b[0m\n\u001b[1;32m     11\u001b[0m s3_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(prefix, relative_path)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Ensure Unix-style paths for S3\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Upload the file to S3\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43ms3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ms3_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbucket_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms3_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/boto3/s3/inject.py:145\u001b[0m, in \u001b[0;36mupload_file\u001b[0;34m(self, Filename, Bucket, Key, ExtraArgs, Callback, Config)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Upload a file to an S3 object.\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03mUsage::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03m    transfer.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m S3Transfer(\u001b[38;5;28mself\u001b[39m, Config) \u001b[38;5;28;01mas\u001b[39;00m transfer:\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtransfer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbucket\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBucket\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mKey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mExtraArgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/boto3/s3/transfer.py:377\u001b[0m, in \u001b[0;36mS3Transfer.upload_file\u001b[0;34m(self, filename, bucket, key, callback, extra_args)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;66;03m# If a client error was raised, add the backwards compatibility layer\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;66;03m# that raises a S3UploadFailedError. These specific errors were only\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;66;03m# ever thrown for upload_parts but now can be thrown for any related\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;66;03m# client error.\u001b[39;00m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClientError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 377\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m S3UploadFailedError(\n\u001b[1;32m    378\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to upload \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    379\u001b[0m             filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([bucket, key]), e\n\u001b[1;32m    380\u001b[0m         )\n\u001b[1;32m    381\u001b[0m     )\n",
      "\u001b[0;31mS3UploadFailedError\u001b[0m: Failed to upload ./local_data/movie/processed_data/movie_users.parquet/.part-00000-221c4992-6444-4e02-8fc4-ce406aa3a911-c000.snappy.parquet.crc to processed-data/movie/.part-00000-221c4992-6444-4e02-8fc4-ce406aa3a911-c000.snappy.parquet.crc: An error occurred (NoSuchBucket) when calling the PutObject operation: The specified bucket does not exist"
     ]
    }
   ],
   "source": [
    "store_processed_parquet(MOVIE_USERS_PARQUET_PATH, prefix='movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b04ef0f6-7393-4708-a197-3f7fba8757b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_processed_parquet(MOVIE_CONTENT_PARQUET_PATH, prefix='movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "54575aae-54a1-4254-8523-a0b6f6d43a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GC had this code to enrich movie_users, I don't see the use yet so I am leaving this commented out.\n",
    "# movie_users = movie_users.join(movie_content, ['content_id'],'left')\n",
    "# movie_users.repartition(1).write.mode('overwrite').parquet(MOVIE_CONTENT_PARQUET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3dec0224-cb06-4811-a349-d4401744ab63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema = ArrayType(\n",
    "#     StructType([\n",
    "#         StructField(\"author\", StringType(), True),\n",
    "#         StructField(\"author_details\", StructType([\n",
    "#             StructField(\"rating\", StringType(), True)\n",
    "#         ]), True),\n",
    "#         StructField(\"created_at\", StringType(), True),\n",
    "#     ])\n",
    "# )\n",
    "\n",
    "# movie_users = spark.read.parquet(MOVIE_BASE_PARQUET_PATH)\\\n",
    "#           .filter(length(\"results\")>2)\\\n",
    "#           .withColumn(\"results_test\", col('results'))\\\n",
    "#           .withColumn(\"results_parsed\", from_json(col(\"results_test\"), schema))\\\n",
    "#           .withColumn(\"result_exploded\", explode(col(\"results_parsed\")))\\\n",
    "#           .withColumn('result_exploded', col(\"result_exploded\").cast(StringType()))\n",
    "\n",
    "# split_col = split(movie_users['result_exploded'], ', ')\n",
    "\n",
    "# movie_users = movie_users.withColumn('author', split_col.getItem(0)) \\\n",
    "#            .withColumn('author', expr(\"substring(author,2, length(author) -1)\")) \\\n",
    "#            .withColumn('rating', split_col.getItem(1)) \\\n",
    "#            .withColumn(\"rating\", expr(\"substring(rating, 2, length(rating) - 2)\"))\\\n",
    "#            .withColumn(\"rating\", col('rating').cast(DoubleType()))\\\n",
    "#            .withColumn('rating_date', split_col.getItem(2))\\\n",
    "#            .withColumn('rating_date', expr(\"substring(rating_date,1, length(rating_date) -1)\"))\\\n",
    "#            .withColumn(\"rating_date\", to_date(col(\"rating_date\"), \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\"))\\\n",
    "#            .select(col('author').alias('user_id'), lit('movie').alias('type'), col('id').alias('content_id').cast(StringType()), 'rating', 'rating_date')\n",
    "\n",
    "# movie_content = spark.read.parquet(MOVIE_BASE_INFO_PATH)\\\n",
    "#               .select(col('id').alias('content_id'),col('overview').alias('description'), col('release_date').alias('release_year'))\\\n",
    "#               .withColumn('release_year', substring(\"release_year\", 1, 4))\n",
    "\n",
    "# # movie_content.repartition(1).write.mode('overwrite').parquet(\"./parsed_data/movies_descript.parquet\")\n",
    "\n",
    "# # Re-lectura\n",
    "\n",
    "# # dfIni = spark.read.parquet(\"./parsed_data/movies_user.parquet\")\n",
    "# # dfDesc = spark.read.parquet(\"./parsed_data/movies_descript.parquet\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552ffe46-8b53-4490-abe8-d3ac5843f259",
   "metadata": {},
   "source": [
    "## Anime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c202fc57-429e-4dfe-b83e-6640ac4387a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANIME_BASE_CONTENT_PATH = './local_data/anime/info'\n",
    "ANIME_BASE_USERS_PATH = './local_data/anime/user_info'\n",
    "ANIME_TEMP_PARQUET_PATH = './local_data/anime/temp'\n",
    "ANIME_USERS_PARQUET_PATH = \"./local_data/anime/processed_data/anime_users.parquet\"\n",
    "ANIME_CONTENT_PARQUET_PATH = \"./local_data/anime/processed_data/anime_content.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09ef7daa-6645-44f5-adc3-d6cea024d251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anime_users_parquet():\n",
    "    if not os.path.exists(ANIME_USERS_PARQUET_PATH):\n",
    "        os.makedirs(ANIME_USERS_PARQUET_PATH)\n",
    "    path_for_anime_lists = ANIME_BASE_USERS_PATH\n",
    "    user_anime_lists_paths = os.listdir(path_for_anime_lists)\n",
    "    \n",
    "    df = spark.read.json(\n",
    "        path = [f'{path_for_anime_lists}/{i}' for i in user_anime_lists_paths],\n",
    "        multiLine = True, \n",
    "        mode = 'DROPMALFORMED'\n",
    "    ).withColumn('file_name', input_file_name()).select(\n",
    "        from_unixtime(col('updated_at')).alias('rating_date'),\n",
    "        col('score').alias('rating'),\n",
    "        col('anime_id').alias('content_id'),\n",
    "        regexp_extract(col('file_name'), '\\/([^\\/]+)\\.json$', 1).alias('user_id'),\n",
    "    )\\\n",
    "    .withColumn('type', lit('anime'))\\\n",
    "    .coalesce(1).write.mode('overwrite').parquet(ANIME_USERS_PARQUET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "25a96514-e91c-4e8b-a553-aea90f892a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anime_content_parquet():\n",
    "    if not os.path.exists(ANIME_CONTENT_PARQUET_PATH):\n",
    "        os.makedirs(ANIME_CONTENT_PARQUET_PATH)\n",
    "\n",
    "    if not os.path.exists(ANIME_TEMP_PARQUET_PATH):\n",
    "        os.makedirs(ANIME_TEMP_PARQUET_PATH)\n",
    "    \n",
    "    path_for_animes = ANIME_BASE_CONTENT_PATH\n",
    "    #TODO not limit it\n",
    "    anime_paths = os.listdir(path_for_animes)[:2000]\n",
    "    \n",
    "    batch_size = 1000\n",
    "    cnt = 0\n",
    "    \n",
    "    while len(anime_paths) > cnt * batch_size :\n",
    "        df = spark.read.json(\n",
    "            path = [f'{path_for_animes}/{i}' for i in anime_paths][cnt * batch_size: (cnt + 1) * batch_size],\n",
    "            multiLine = True, \n",
    "            mode = 'DROPMALFORMED'\n",
    "        )\\\n",
    "        .dropna(subset=['data.aired.prop.from.year'])\n",
    "        df.write.mode('overwrite').parquet(f'{ANIME_TEMP_PARQUET_PATH}/{cnt}')\n",
    "        cnt += 1\n",
    "    \n",
    "    parquet_files_path = ANIME_TEMP_PARQUET_PATH\n",
    "    parquet_files = os.listdir(parquet_files_path)\n",
    "    df = spark.read.parquet(*[f'{parquet_files_path}/{i}' for i in parquet_files])\n",
    "    df.select(\n",
    "        col('data.synopsis').alias('description'),\n",
    "        col('data.title').alias('title'),\n",
    "        col('data.mal_id').cast(StringType()).alias('content_id'),\n",
    "        col('data.aired.prop.from.year').alias('release_year')\n",
    "    )\\\n",
    "    .withColumn('type', lit('anime'))\\\n",
    "    .coalesce(1).write.mode('overwrite').parquet(ANIME_CONTENT_PARQUET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "067c01a7-ea3e-425a-a685-1b34f66ad405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anime_users_df():\n",
    "    anime_users = spark.read.parquet(ANIME_USERS_PARQUET_PATH)\n",
    "    return anime_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "37ff4893-4464-420f-a3c4-cca1b9b83de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anime_content_df():\n",
    "    anime_content = spark.read.parquet(ANIME_CONTENT_PARQUET_PATH)\n",
    "    return anime_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f5699c63-31a2-48ff-8b27-547a307fc269",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_anime_users_parquet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "469e1076-4a48-42a7-a1c3-e8548d76822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_anime_content_parquet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "606ecb67-932d-4eb6-9510-d964cc71a7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+----------+---------+-----+\n",
      "|        rating_date|rating|content_id|  user_id| type|\n",
      "+-------------------+------+----------+---------+-----+\n",
      "|2023-03-17 00:35:34|     0|       918|Nabil_967|anime|\n",
      "|2023-08-13 12:15:08|    10|        21|Nabil_967|anime|\n",
      "|2022-07-05 23:35:51|     0|     48583|Nabil_967|anime|\n",
      "|2023-07-29 14:19:26|     9|     52034|Nabil_967|anime|\n",
      "|2022-03-01 19:53:09|     5|     41380|Nabil_967|anime|\n",
      "+-------------------+------+----------+---------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- rating_date: string (nullable = true)\n",
      " |-- rating: long (nullable = true)\n",
      " |-- content_id: long (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "anime_users = get_anime_users_df()\n",
    "anime_users.show(5)\n",
    "anime_users.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7b4bf4f5-5b60-4635-8c4a-405217a608c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+------------+-----+\n",
      "|         description|               title|content_id|release_year| type|\n",
      "+--------------------+--------------------+----------+------------+-----+\n",
      "|Kokone Morikawa h...|Hirune Hime: Shir...|     33204|        2017|anime|\n",
      "|2199 AD. Yamato t...|Uchuu Senkan Yama...|     23249|        2014|anime|\n",
      "|During their ques...|InuYasha Movie 1:...|       452|        2001|anime|\n",
      "|Fortune smiles on...|InuYasha Movie 2:...|       450|        2002|anime|\n",
      "|In the Kingdom of...|Berserk: Ougon Ji...|     10218|        2012|anime|\n",
      "+--------------------+--------------------+----------+------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- description: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- content_id: string (nullable = true)\n",
      " |-- release_year: long (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "anime_content = get_anime_content_df()\n",
    "anime_content.show(5)\n",
    "anime_content.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2849aa30-fa44-4684-b858-fe8f53b51a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_processed_parquet(ANIME_USERS_PARQUET_PATH, prefix='anime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fbbf06c6-d114-4845-a2a6-32aa3bc7dda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_processed_parquet(ANIME_CONTENT_PARQUET_PATH, prefix='anime')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a972e3-9f27-40a7-baac-c8745c15b9c5",
   "metadata": {},
   "source": [
    "## Videogames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "295f45c9-0a19-420b-9e3d-344c877e992d",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEOGAME_BASE_SUMMARIES_PATH = './local_data/videogame/player_profile.json'\n",
    "VIDEOGAME_BASE_PROFILES_PATH = './local_data/videogame/games_played.json'\n",
    "VIDEOGAME_BASE_GAMES_PATH = './local_data/videogame/steam_games.json'\n",
    "VIDEOGAME_USERS_PARQUET_PATH = \"./local_data/videogame/processed_data/v_users.parquet\"\n",
    "VIDEOGAME_CONTENT_PARQUET_PATH = \"./local_data/videogame/processed_data/videogame_content.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "668148b7-efe6-49e0-9351-05cd90cd10d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_videogame_users_parquet():\n",
    "    # Load player_summaries.json\n",
    "    with open(VIDEOGAME_BASE_SUMMARIES_PATH, 'r') as f:\n",
    "        player_summaries_data = json.load(f)\n",
    "    \n",
    "    # Load steam_profiles.json\n",
    "    with open(VIDEOGAME_BASE_PROFILES_PATH, 'r') as f:\n",
    "        steam_profiles_data = json.load(f)\n",
    "    \n",
    "    # Initialize list to store data\n",
    "    common_rows = []\n",
    "    \n",
    "    # Process data from steam_profiles_data\n",
    "    for steam_profiles in steam_profiles_data:\n",
    "        steamid = list(steam_profiles.keys())[0]\n",
    "        games = steam_profiles[steamid]\n",
    "        player_summary = next((summary for summary in player_summaries_data if steamid in summary), None)\n",
    "        # Check if the player summary data is available and not empty\n",
    "        if player_summary and player_summary[steamid]:\n",
    "            personaname = player_summary[steamid].get('personaname', 'Unknown')\n",
    "            for game in games:\n",
    "                appid = game['appid']\n",
    "                playtime_forever = game['playtime_forever']\n",
    "                if playtime_forever > 0:  # Skip if playtime_forever is 0\n",
    "                    common_rows.append({'user_id': personaname, 'type': 'videogame', 'content_id': appid, 'temp_rating': playtime_forever})\n",
    "    \n",
    "    # Create Spark DataFrame\n",
    "    common_df = spark.createDataFrame(common_rows)\n",
    "    \n",
    "    # Calculate max playtime_forever for each user_id\n",
    "    max_playtime = common_df.groupBy('user_id').agg(spark_max('temp_rating').alias('max_temp_rating'))\n",
    "    \n",
    "    # Join max_playtime with common_df to calculate normalized ratings\n",
    "    common_df = common_df.join(max_playtime, on='user_id')\n",
    "    common_df = common_df.withColumn('rating', (col('temp_rating') / col('max_temp_rating')) * 10)\n",
    "    \n",
    "    # Apply ceiling to the ratings\n",
    "    common_df = common_df.withColumn('rating', ceil(col('rating')))\n",
    "    \n",
    "    # Drop the 'temp_rating' and 'max_temp_rating' columns\n",
    "    common_df = common_df.drop('temp_rating', 'max_temp_rating')\n",
    "    \n",
    "    # Add a new column 'rating_date' filled with null values\n",
    "    common_df = common_df.withColumn('rating_date', lit(None).cast('string'))\n",
    "    \n",
    "    # Display the Spark DataFrame\n",
    "    # common_df.show(10)\n",
    "    \n",
    "    # Take a sample of the data. Comment or uncomment \n",
    "    # sample_df = common_df.sample(withReplacement=False, fraction=0.001)\n",
    "    # sample_df.write.parquet('sample_steam_users.parquet')\n",
    "    \n",
    "    # Save DataFrame as Parquet file\n",
    "    common_df.write.mode('overwrite').parquet(VIDEOGAME_USERS_PARQUET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "66b7b209-82bf-40cb-b2a6-d5e336012ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_videogame_content_parquet():\n",
    "    # Load the dataset from games.json\n",
    "    dataset = {}\n",
    "    if os.path.exists(VIDEOGAME_BASE_GAMES_PATH):\n",
    "        with open(VIDEOGAME_BASE_GAMES_PATH, 'r', encoding='utf-8') as fin:\n",
    "            text = fin.read()\n",
    "            if len(text) > 0:\n",
    "                dataset = json.loads(text)\n",
    "    \n",
    "    # Initialize list to store data\n",
    "    rows = []\n",
    "    \n",
    "    # Extract the relevant data\n",
    "    for app_id, game_info in dataset.items():\n",
    "        name = game_info.get('name', '')\n",
    "        release_date = game_info.get('release_date', '')\n",
    "        # Extract the year from the release_date\n",
    "        if release_date:\n",
    "            release_year = release_date.split()[-1]\n",
    "        else:\n",
    "            release_year = ''\n",
    "        description = game_info.get('detailed_description', '')\n",
    "    \n",
    "        rows.append(Row(content_id=app_id, title=name, release_year=release_year, description=description))\n",
    "    \n",
    "    # Create Spark DataFrame\n",
    "    df = spark.createDataFrame(rows)\n",
    "    df = df.withColumn('type', lit('videogame'))\n",
    "    \n",
    "    # Display the first few rows of the DataFrame\n",
    "    # df.show()\n",
    "    \n",
    "    # Save DataFrame as Parquet file\n",
    "    # df.write.parquet(OUTPUT_PARQUET_FILE)\n",
    "    df.write.mode('overwrite').parquet(VIDEOGAME_CONTENT_PARQUET_PATH)\n",
    "    # Stop Spark session\n",
    "    # spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "19a6ce98-04fe-42fc-b1fa-4ec8fc13d18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_videogame_users_df():\n",
    "    videogame_users = spark.read.parquet(VIDEOGAME_USERS_PARQUET_PATH)\n",
    "    return videogame_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9712b582-1652-4722-8701-5984d90479af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_videogame_content_df():\n",
    "    videogame_content = spark.read.parquet(VIDEOGAME_CONTENT_PARQUET_PATH)\n",
    "    return videogame_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "038c56d7-e375-4acc-b707-014bbcc593dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_videogame_users_parquet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "67b67d64-1c9d-4fe9-895d-2c05e2829dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_videogame_content_parquet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "01ed97db-dcef-41e5-b239-9f10bc4488e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+------+-----------+\n",
      "|user_id|content_id|     type|rating|rating_date|\n",
      "+-------+----------+---------+------+-----------+\n",
      "|   Fooo|       300|videogame|     1|       null|\n",
      "|   Fooo|      4000|videogame|     1|       null|\n",
      "|   Fooo|      2600|videogame|     1|       null|\n",
      "|   Fooo|       220|videogame|     1|       null|\n",
      "|   Fooo|       500|videogame|     1|       null|\n",
      "+-------+----------+---------+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- content_id: long (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- rating: long (nullable = true)\n",
      " |-- rating_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "videogame_users = get_videogame_users_df()\n",
    "videogame_users.show(5)\n",
    "videogame_users.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "29b5823e-8c42-4339-b4f6-75770a185aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------+--------------------+---------+\n",
      "|content_id|               title|release_year|         description|     type|\n",
      "+----------+--------------------+------------+--------------------+---------+\n",
      "|    974520|        Ultimo Reino|        2020|Features: -World ...|videogame|\n",
      "|   1331460|   Super Gloves Hero|        2020|Click your way th...|videogame|\n",
      "|   1242910|The House in the ...|        2020|The House in the ...|videogame|\n",
      "|   1990390|  Splitgate Playtest|        2022|                    |videogame|\n",
      "|   1558330|      Mall of Mayhem|        2022|Mall of Mayhem is...|videogame|\n",
      "+----------+--------------------+------------+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- content_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- release_year: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "videogame_content = get_videogame_content_df()\n",
    "videogame_content.show(5)\n",
    "videogame_content.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "40d5fe6f-1d9e-40fd-923e-da760c5dfa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_processed_parquet(VIDEOGAME_USERS_PARQUET_PATH, prefix='videogame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "37193680-a69f-480d-bbec-4bbe485797a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_processed_parquet(VIDEOGAME_CONTENT_PARQUET_PATH, prefix='videogame')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac3ee52-5f72-4413-95fd-9900ce409979",
   "metadata": {},
   "source": [
    "# Merging all content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6d123d9e-db41-4014-93c7-5c35596ed28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- content_id: string (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- rating_date: string (nullable = true)\n",
      "\n",
      "+-----------+---------+----------+------+-------------------+\n",
      "|    user_id|     type|content_id|rating|        rating_date|\n",
      "+-----------+---------+----------+------+-------------------+\n",
      "|zefquaavius|boardgame|    322232|   6.0|2023-08-01 14:52:32|\n",
      "|zefquaavius|boardgame|    296402|   8.0|2023-08-02 14:18:24|\n",
      "|zefquaavius|boardgame|    336537|  null|2023-08-02 14:18:38|\n",
      "|zefquaavius|boardgame|    314445|  null|2023-08-02 14:18:54|\n",
      "|zefquaavius|boardgame|    296404|  null|2023-08-02 14:19:11|\n",
      "|zefquaavius|boardgame|    296406|  null|2023-08-01 14:56:42|\n",
      "|zefquaavius|boardgame|    322429|  null|2023-08-02 14:19:32|\n",
      "|zefquaavius|boardgame|    309782|   6.0|2023-08-01 14:57:01|\n",
      "|zefquaavius|boardgame|    314446|  null|2023-08-02 14:19:57|\n",
      "|zefquaavius|boardgame|    296407|  null|2023-08-01 14:57:31|\n",
      "|zefquaavius|boardgame|    296423|  null|2023-08-01 14:57:46|\n",
      "|zefquaavius|boardgame|    296411|  null|2023-08-02 14:20:11|\n",
      "|zefquaavius|boardgame|    321710|  null|2023-08-02 14:20:23|\n",
      "|zefquaavius|boardgame|    309530|   4.0|2023-08-01 14:58:03|\n",
      "|zefquaavius|boardgame|    314444|  null|2023-08-02 14:20:37|\n",
      "|zefquaavius|boardgame|    336618|   7.0|2023-08-01 14:58:12|\n",
      "|zefquaavius|boardgame|    296272|  null|2023-08-02 14:21:09|\n",
      "|zefquaavius|boardgame|    309523|   7.0|2023-08-02 14:21:22|\n",
      "|zefquaavius|boardgame|    309658|   6.0|2023-08-01 15:00:31|\n",
      "|zefquaavius|boardgame|    324093|   7.0|2023-08-01 15:00:39|\n",
      "+-----------+---------+----------+------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merged_users = (\n",
    "    boardgame_users\n",
    "    .union(movie_users.select(['user_id', 'type', 'content_id', 'rating', 'rating_date']))\n",
    "    .union(anime_users.select(['user_id', 'type', 'content_id', 'rating', 'rating_date']))\n",
    "    .union(videogame_users.select(['user_id', 'type', 'content_id', 'rating', 'rating_date']))\n",
    "    .withColumn('user_id', trim(lower(col('user_id'))))\n",
    "    .withColumn('type', lower(col('type')))\n",
    ")\n",
    "\n",
    "merged_users.printSchema()\n",
    "merged_users.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bb696fb3-8bbc-4d85-be1d-a6b900529517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------+--------------------+---------+\n",
      "|content_id|         description|release_year|               title|     type|\n",
      "+----------+--------------------+------------+--------------------+---------+\n",
      "|     13191|From the publishe...|        1980|  Super Quintillions|boardgame|\n",
      "|      2120|A very simple gam...|        2000|Looney Tunes Trad...|boardgame|\n",
      "|     13005|Dance of Ibexes, ...|        2004|     Dance of Ibexes|boardgame|\n",
      "|      1425|This solitaire wa...|        1987| Raid on St. Nazaire|boardgame|\n",
      "|      1096|Party trivia game...|        1999|           Four Real|boardgame|\n",
      "|     13262|The players have ...|        1977|        Spider & Fly|boardgame|\n",
      "|      1147|The Official Towe...|        1992|Outrage! Steal th...|boardgame|\n",
      "|       142|Players compete w...|        1999|                Vino|boardgame|\n",
      "|     11899|A trivia game tha...|        1988|        Le Docte Rat|boardgame|\n",
      "|      1325|A children's tric...|        1989|        Wild Pirates|boardgame|\n",
      "|       137|This frantic game...|        1994|       Pass the Bomb|boardgame|\n",
      "|        41|In this Sid Sacks...|        1980|          Can't Stop|boardgame|\n",
      "|     12105|From the back cov...|        1998|Full Thrust Fleet...|boardgame|\n",
      "|       128|It's really diffi...|        1983|       Take it Easy!|boardgame|\n",
      "|     10293|<Translated from ...|        1981|En busca del Impe...|boardgame|\n",
      "|       217|In one of his sil...|        1989|          A la carte|boardgame|\n",
      "|      1210|In Cults Across A...|        1998|Cults Across America|boardgame|\n",
      "|     12199|The Dragon Hordes...|        2004|Warriors: Dragon ...|boardgame|\n",
      "|      2049|Target® is a rumm...|        1997|              Target|boardgame|\n",
      "|      1960|Players pieces ar...|        1985|           Last Word|boardgame|\n",
      "+----------+--------------------+------------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merged_content = (\n",
    "    boardgame_content.limit(1000)\n",
    "    .unionByName(movie_content.limit(1000).select(['content_id', 'description', 'release_year', 'type','title']))\n",
    "    .unionByName(anime_content.limit(1000).select(['content_id', 'description', 'release_year', 'type','title']))\n",
    "    .unionByName(videogame_content.limit(1000).select(['content_id', 'description', 'release_year', 'type','title']))\n",
    "    # .withColumn('user_id', trim(lower(col('user_id'))))\n",
    "    # .withColumn('type', lower(col('type')))\n",
    ")\n",
    "\n",
    "merged_content.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0466aa78-16b0-4c5f-854e-ddc6a7aec61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b9a3acb8-9578-4a41-b684-ff0b3dde23b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|     type| count|\n",
      "+---------+------+\n",
      "|    anime|388227|\n",
      "|videogame|289674|\n",
      "|boardgame|258496|\n",
      "|    movie|    36|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    merged_users\n",
    "    .select('type')\n",
    "    .groupBy(col('type'))\n",
    "    .count()\n",
    "    .sort(col('count'), ascending=False)\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "aff487b1-213b-4716-a9d6-9c2361345e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|     type|count|\n",
      "+---------+-----+\n",
      "|boardgame| 1000|\n",
      "|    movie| 1000|\n",
      "|    anime| 1000|\n",
      "|videogame| 1000|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    merged_content\n",
    "    .select('type')\n",
    "    .groupBy(col('type'))\n",
    "    .count()\n",
    "    .sort(col('count'), ascending=False)\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1359205c-5e90-4a31-a8be-b5de37be295e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a6841792-7754-4eed-b50b-96c6acb1e694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|           user_id|count|\n",
      "+------------------+-----+\n",
      "|       zefquaavius| 5547|\n",
      "|      akapastorguy| 4636|\n",
      "|   piston smashed™| 4615|\n",
      "|        slashbunny| 3447|\n",
      "|          naarnold| 3212|\n",
      "|       invader_bzz| 3190|\n",
      "|             tydel| 3171|\n",
      "|         adrimetum| 2959|\n",
      "|          doccabet| 2840|\n",
      "|            huffa2| 2434|\n",
      "|              muyf| 2372|\n",
      "|           domi123| 2243|\n",
      "|       donnie lama| 2224|\n",
      "|          fitadine| 2158|\n",
      "|kreikkaturkulainen| 2004|\n",
      "|         doomfarer| 1994|\n",
      "|            d0gb0t| 1974|\n",
      "|    saxophonechapa| 1951|\n",
      "|            landru| 1922|\n",
      "|     thechrisglass| 1919|\n",
      "+------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merged_users.groupBy(col('user_id')).count().sort(col('count'), ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e19086a1-8b81-45dc-9e50-4543478a86fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|       user_id|count|\n",
      "+--------------+-----+\n",
      "|  luckyoneputt|    1|\n",
      "|     eagle1207|    1|\n",
      "|        sqicky|    1|\n",
      "|spacebutterfly|    1|\n",
      "|        r4yn3x|    1|\n",
      "|   soxrivotril|    1|\n",
      "|    thetheredk|    1|\n",
      "|      satertek|    1|\n",
      "|       hukaers|    1|\n",
      "|   monkayylmao|    1|\n",
      "|    scarlsberg|    1|\n",
      "|           gua|    1|\n",
      "|      jettison|    1|\n",
      "|   simonknight|    1|\n",
      "|          holz|    1|\n",
      "|        buddah|    1|\n",
      "|        max667|    1|\n",
      "|    frankbouch|    1|\n",
      "|   neonjedesis|    1|\n",
      "|     reilove69|    1|\n",
      "+--------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    merged_users\n",
    "    .select('user_id', 'type')\n",
    "    .withColumn('user_id', trim(regexp_replace(lower(col('user_id')), '[^a-zA-Z0-9]', '')))\n",
    "    .distinct()\n",
    "    .groupBy(col('user_id'))\n",
    "    .count()\n",
    "    .sort(col('count'), ascending=False)\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0ca9acef-45c6-4711-bda9-660e6f00f0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|     type|count|\n",
      "+---------+-----+\n",
      "|videogame|  251|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    merged_users\n",
    "    .filter(merged_users['user_id'] == 'daimyo')\n",
    "    .groupBy(col('type'))\n",
    "    .count()\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553f0ff2-4c0d-41b5-a381-cc7e165381b6",
   "metadata": {},
   "source": [
    "# Yake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96df27f-694e-4f9a-b038-be04f00ba9ad",
   "metadata": {},
   "source": [
    "## RDD Way\n",
    "It took 2 min and 7 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a8a43655-f4a2-41ae-85f5-a6bbe6c847e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = merged_content\n",
    "# rddK = df.rdd.map(lambda x: (x['content_id'], get_kw(x['description'])))\n",
    "# rddK = spark.createDataFrame(rddK).select(col('_1').alias('content_id'), col('_2').alias('keyword'))\n",
    "# dfK = (rddK.withColumn(\"keyword_1\", expr(\"keyword[0]\"))\n",
    "#                 .withColumn(\"keyword_2\", expr(\"keyword[1]\"))\n",
    "#                 .withColumn(\"keyword_3\", expr(\"keyword[2]\"))\n",
    "#                 .withColumn(\"keyword_4\", expr(\"keyword[3]\"))\n",
    "#                 .withColumn(\"keyword_5\", expr(\"keyword[4]\"))\n",
    "#                 .select('content_id','keyword_1','keyword_2','keyword_3','keyword_4','keyword_5' )\n",
    "#       )\n",
    "# dfK.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4c40f95c-ed58-44df-8f0d-2f9a76a3b30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 4.29 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# dfK.write.mode('overwrite').parquet('./foo/keywords_full_rdd')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bc79eb-3f43-4eee-8c60-fa6ea8432978",
   "metadata": {},
   "source": [
    "- [ ] connect directly spark to neo4j (using the right connector)\n",
    "- [ ] maybe provide some analytics about the users' profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0a6d4b0d-29ee-48df-af1c-ef0070bee2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what the RS does for the NULL values.\n",
    "# - We could impute something, like the average score the user gives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d7a78c-980f-49d9-97c5-437d6ccc8916",
   "metadata": {},
   "source": [
    "## Dataframe UDF way\n",
    "It took 2 min and 5 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a966360a-1481-4d68-8c83-8be9b75b3114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+--------------------+--------------------+--------------------+\n",
      "|           keyword_0|        keyword_1|           keyword_2|           keyword_3|           keyword_4|\n",
      "+--------------------+-----------------+--------------------+--------------------+--------------------+\n",
      "|   publisher website|     quintillions|              pieces|       expansion set|             website|\n",
      "|              actors|            scene|          loony tune|              scenes|               color|\n",
      "|                tile|   der hornochsen|            tanz der|               tiles|                 row|\n",
      "|nazi-occupied french|      french port|   solitaire wargame|   wargame simulates|        british raid|\n",
      "|        party trivia|      trivia game|    unlimited number|               teams|               party|\n",
      "|          spider web|            flies|              spider|          flies back|                 web|\n",
      "|          board game|   official tower|        london board|         white tower|               tower|\n",
      "|               italy|  players compete|hippodice spielea...|           vineyards|           das spiel|\n",
      "|  educational stages|      theme based|collège saint-lau...|     winning college|université gens-b...|\n",
      "|children trick-ta...|    racing pirate|   trick-taking card|           card game|        pirate ships|\n",
      "|  wonderful exercise|     frantic game|         family game|        year denmark|      word-summoning|\n",
      "|         sid sackson|  sackson classic|combinations tact...|                 sid|             sackson|\n",
      "|         full thrust|tactical starship|     starship combat|          back cover|    starship designs|\n",
      "| succinctly describe|             easy|              number|               tiles|               board|\n",
      "|      empire returns|        magic eye|          translated|           cobra men|        cobra island|\n",
      "|  karl-heinz schmiel|    schmiel casts|     culinary skills|semi-psychotic cooks|    cooks attempting|\n",
      "|       united states|    entire united|             america|               cults|       standard game|\n",
      "|    hordes expansion|    warriors adds|       dragon hordes|        attack cards|         small pages|\n",
      "|              target|     target cards|               cards|               melds|          rummy game|\n",
      "|         wild spaces|   players pieces|              player|                grid|              spaces|\n",
      "+--------------------+-----------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 11.3 ms, sys: 638 µs, total: 11.9 ms\n",
      "Wall time: 22.9 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# merged_content.withColumn('yake',get_kw(col('description')))\\\n",
    "# .select(\n",
    "#     expr(\"yake[0]\").alias('keyword_0'),\n",
    "#     expr(\"yake[1]\").alias('keyword_1'),\n",
    "#     expr(\"yake[2]\").alias('keyword_2'),\n",
    "#     expr(\"yake[3]\").alias('keyword_3'),\n",
    "#     expr(\"yake[4]\").alias('keyword_4'),\n",
    "# )\\\n",
    "# .show()\n",
    "# .write.mode('overwrite').parquet('./foo/keywords_full_not_exploded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e5f4d4-7fc1-4cb8-b969-ac4781e67630",
   "metadata": {},
   "source": [
    "## Explode way \n",
    "because it is easier to upload in neo4j\n",
    "Takes a lot longer, 3 min 53 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c96559bd-83e9-4af2-b393-15021d7966f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 11.4 ms, total: 11.4 ms\n",
      "Wall time: 26.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "\n",
    "def get_kw(text):\n",
    "   kw_extractor = yake.KeywordExtractor(\n",
    "       lan='en',\n",
    "       n=2,  # Max n-gram size\n",
    "       top=5  # Number of keywords\n",
    "   )\n",
    "    \n",
    "   return list(map(lambda x: str.lower(x[0]) if x else '', kw_extractor.extract_keywords(text)))\n",
    "\n",
    "get_kw = udf(get_kw, ArrayType(StringType()))\n",
    "\n",
    "merged_content.withColumn('yake',get_kw(col('description')))\\\n",
    ".select(\n",
    "    explode(col('yake')).alias('keywords'),\n",
    "    'description',\n",
    "    'release_year',\n",
    "    'type',\n",
    "    'title',\n",
    "    'content_id'\n",
    ").write.mode('overwrite').parquet('./foo/keywords_full_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce826eeb-01c7-48f7-8f05-beef72d44f23",
   "metadata": {},
   "source": [
    "# Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "00605030-10bc-47fa-9c53-6106c96f8d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEO4J_URL = 'bolt://neo4j:7687'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "53c682f6-b40c-4115-af47-45683eb72e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting graphdatascience\n",
      "  Using cached graphdatascience-1.10-py3-none-any.whl (1.6 MB)\n",
      "Collecting multimethod<2.0,>=1.0 (from graphdatascience)\n",
      "  Using cached multimethod-1.11.2-py3-none-any.whl (10 kB)\n",
      "Collecting neo4j<6.0,>=4.4.2 (from graphdatascience)\n",
      "  Using cached neo4j-5.20.0-py3-none-any.whl\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from graphdatascience) (2.0.2)\n",
      "Requirement already satisfied: pyarrow<16.0,>=11.0 in /opt/conda/lib/python3.11/site-packages (from graphdatascience) (12.0.0)\n",
      "Collecting textdistance<5.0,>=4.0 (from graphdatascience)\n",
      "  Using cached textdistance-4.6.2-py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.0 in /opt/conda/lib/python3.11/site-packages (from graphdatascience) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions<5.0,>=4.0 in /opt/conda/lib/python3.11/site-packages (from graphdatascience) (4.6.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from graphdatascience) (2.31.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.11/site-packages (from neo4j<6.0,>=4.4.2->graphdatascience) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas<3.0,>=1.0->graphdatascience) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas<3.0,>=1.0->graphdatascience) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.11/site-packages (from pandas<3.0,>=1.0->graphdatascience) (1.24.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->graphdatascience) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->graphdatascience) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->graphdatascience) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->graphdatascience) (2023.5.7)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->graphdatascience) (1.16.0)\n",
      "Installing collected packages: textdistance, neo4j, multimethod, graphdatascience\n",
      "Successfully installed graphdatascience-1.10 multimethod-1.11.2 neo4j-5.20.0 textdistance-4.6.2\n"
     ]
    }
   ],
   "source": [
    "!pip install graphdatascience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b5e999e1-d3f3-4420-9c23-6c7ebc3cc98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphdatascience import GraphDataScience\n",
    "gds = GraphDataScience(\"neo4j://neo4j\",aura_ds=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31c679f-247b-4af2-8037-9fb5cad49167",
   "metadata": {},
   "source": [
    "## Uploads Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "351c19ff-ecd8-4637-80d4-358880a14ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gds.run_cypher('CREATE INDEX content_id FOR (n:content) ON (n.content_id)')\n",
    "gds.run_cypher('CREATE INDEX keywords FOR (n:keyword) ON (n.keyword)')\n",
    "gds.run_cypher('CREATE INDEX users_id FOR (n:users) ON (n.users_id)')\n",
    "gds.run_cypher('CREATE INDEX content_type FOR (n:content) ON (n.type)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "76ad0a9d-b37e-461f-a30a-9a319f29eae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "kinds = ['boardgame','movie','anime','videogame']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ff72bbb4-de20-4ceb-ac34-4e3b6293fffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.72 ms, sys: 3.56 ms, total: 12.3 ms\n",
      "Wall time: 44.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "kw_df = spark.read.parquet('./foo/keywords_full_2')\n",
    "for kind in kinds:\n",
    "    (\n",
    "    kw_df.filter(col('type') == kind).write\n",
    "      # // Create new relationships\n",
    "      .mode('Append')\n",
    "      .format(\"org.neo4j.spark.DataSource\")\n",
    "      # // Assign a type to the relationships\n",
    "      .option(\"relationship\", \"has_keyword\")\n",
    "      # // Use `keys` strategy\n",
    "      .option(\"relationship.save.strategy\", \"keys\")\n",
    "      # // Overwrite source nodes and assign them a label\n",
    "      .option(\"relationship.source.save.mode\", \"Overwrite\")\n",
    "      # .option(\"relationship.source.labels\", f\":{kind}\")\n",
    "      .option(\"relationship.source.labels\", ':content')\n",
    "      # // Map the DataFrame columns to node properties\n",
    "      .option(\"relationship.source.node.properties\", \"title,content_id,type\")\n",
    "      # // Node keys are mandatory for overwrite save mode\n",
    "      .option(\"relationship.source.node.keys\", \"content_id,type\")\n",
    "      # // Overwrite target nodes and assign them a label\n",
    "      .option(\"relationship.target.save.mode\", \"Overwrite\")\n",
    "      .option(\"relationship.target.labels\", \":keyword\")\n",
    "      # // Map the DataFrame columns to node properties\n",
    "      .option(\"relationship.target.node.properties\", \"keywords\")\n",
    "      # // Node keys are mandatory for overwrite save mode\n",
    "      .option(\"relationship.target.node.keys\", \"keywords\")\n",
    "      # // Map the DataFrame columns to relationship properties\n",
    "      # .option(\"relationship.properties\", \"quantity,order\")\n",
    "      .option(\"url\", NEO4J_URL)\n",
    "      .save()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60298842-0483-4f9c-9c31-3fa8f48828f7",
   "metadata": {},
   "source": [
    "## Upload users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8b5635c9-0360-4c93-bdaa-3f8cbf58b1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_users = merged_users\\\n",
    ".join(\n",
    "    merged_content, \n",
    "    (merged_content.type == merged_users.type) & (merged_content.content_id == merged_users.content_id), \n",
    "    how='right'\n",
    ").select(['user_id',merged_users['type'],merged_users['content_id'],'rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a8ea1782-eb2e-47a0-b974-cef8d88330e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.8 ms, sys: 5.59 ms, total: 18.4 ms\n",
      "Wall time: 17.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for kind in kinds:\n",
    "    for r_type in ['like','dislike']:\n",
    "        if r_type == 'like':\n",
    "            df = filter_users.filter(col('rating') >= 8)\n",
    "        else :\n",
    "            df = filter_users.filter((col('rating') >= 1) & (col('rating')<8))\n",
    "        (\n",
    "        df.filter(col('type') == kind).write\n",
    "          # // Create new relationships\n",
    "          .mode('Append')\n",
    "          .format(\"org.neo4j.spark.DataSource\")\n",
    "          # // Assign a type to the relationships\n",
    "          .option(\"relationship\", r_type)\n",
    "          # // Use `keys` strategy\n",
    "          .option(\"relationship.save.strategy\", \"keys\")\n",
    "          # // Overwrite source nodes and assign them a label\n",
    "          .option(\"relationship.source.save.mode\", \"Overwrite\")\n",
    "          # .option(\"relationship.source.labels\", f\":{kind}\")\n",
    "          .option(\"relationship.source.labels\", ':content')\n",
    "          # // Map the DataFrame columns to node properties\n",
    "          # .option(\"relationship.source.node.properties\", \"content_id,type\")\n",
    "          # // Node keys are mandatory for overwrite save mode\n",
    "          .option(\"relationship.source.node.keys\", \"content_id,type\")\n",
    "          # // Overwrite target nodes and assign them a label\n",
    "          .option(\"relationship.target.save.mode\", \"Overwrite\")\n",
    "          .option(\"relationship.target.labels\", \":users\")\n",
    "          # // Map the DataFrame columns to node properties\n",
    "          # .option(\"relationship.target.node.properties\", \"user_id,type\")\n",
    "          # // Node keys are mandatory for overwrite save mode\n",
    "          .option(\"relationship.target.node.keys\", \"user_id,type\")\n",
    "          # // Map the DataFrame columns to relationship properties\n",
    "          # .option(\"relationship.properties\", \"rating_date\")\n",
    "          .option(\"url\", NEO4J_URL)\n",
    "          .save()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfc8264-ee89-4756-ac8b-6637aeee89fa",
   "metadata": {},
   "source": [
    "## Neo4j Data Science\n",
    "Con Fé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dbb6f771-d979-4fd9-a8cf-9c2b29100662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "075cfc068d834092b00626bffd6c1f7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading:   0%|          | 0/100 [00:00<?, ?%/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "nodeProjection            {'content': {'label': 'content', 'properties':...\n",
       "relationshipProjection    {'like': {'aggregation': 'DEFAULT', 'orientati...\n",
       "graphName                                              embedding-projection\n",
       "nodeCount                                                             21255\n",
       "relationshipCount                                                     58690\n",
       "projectMillis                                                           590\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Project the graph embedding\n",
    "\n",
    "g0, res = gds.graph.project(\n",
    "    'embedding-projection', \n",
    "    ['content', 'users','keyword' ],\n",
    "    {\n",
    "        'like':{'orientation':'UNDIRECTED'},\n",
    "        # 'dislike':{'orientation':'UNDIRECTED'},\n",
    "        'has_keyword':{'orientation':'UNDIRECTED'},\n",
    "    },\n",
    ")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "120241a7-9962-4bff-bed3-672a1ed5f7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14371705055236816\n",
      "CPU times: user 4.19 ms, sys: 0 ns, total: 4.19 ms\n",
      "Wall time: 144 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "t0 = time.time()\n",
    "# gds.beta.graphSage.mutate(g0, mutateProperty='embedding', embeddingDimension=50, randomSeed=7474, relationshipWeightProperty='weight');\n",
    "# gds.beta.graphSage.mutate(g0, mutateProperty='embedding',modelName='exampleTrainModel');\n",
    "gds.fastRP.mutate(g0, mutateProperty='embedding', embeddingDimension=256, randomSeed=7474);\n",
    "t1 = time.time()\n",
    "print(t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e1839bd1-783d-4511-b2f1-b961d654f654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "writeMillis                           223\n",
       "graphName            embedding-projection\n",
       "nodeProperties                [embedding]\n",
       "propertiesWritten                    6396\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gds.graph.writeNodeProperties(g0, [\"embedding\"], [\"content\",\"users\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4376a0b4-5f62-4c26-81cd-7159d9aa083b",
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = gds.run_cypher('MATCH(n:content) RETURN n.content_id, n.embedding LIMIT 1000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bd86a8b9-4763-40c0-9448-d0c00d3ea8cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sum\n",
       "-0.058583    1\n",
       " 0.553216    1\n",
       "-0.604392    1\n",
       " 1.181782    1\n",
       "-2.006764    1\n",
       "            ..\n",
       "-0.634887    1\n",
       "-0.948896    1\n",
       " 1.932599    1\n",
       "-0.759123    1\n",
       "-1.294192    1\n",
       "Name: count, Length: 1000, dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check, embeddings should be different\n",
    "\n",
    "import seaborn as sns\n",
    "foo['sum'] = foo['n.embedding'].apply(np.sum)\n",
    "foo['sum'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "503fd275-19ae-4c7a-b208-251697d77569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('videogame', '3on3 FreeStyle: Rebound', '1292630'),\n",
       " ('videogame', 'Taco Gun', '842990'),\n",
       " ('videogame', 'ODA', '1142240'),\n",
       " ('videogame', 'Christmas Race 2', '801890'),\n",
       " ('videogame', 'Energy Cycle', '415960'),\n",
       " ('videogame', 'Shining Jump Jump', '2019270')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def get_recommendation_raw(content_id, origin_kind, destination_kind, limit):\n",
    "    '''\n",
    "        given the origin content_id and origin type\n",
    "        give me all the results \n",
    "    '''\n",
    "    result = gds.run_cypher( '''\n",
    "    MATCH (c1:content {content_id:$CONTENT_ID, type:$ORIGIN_KIND})\n",
    "    MATCH (c2:content {type:$DESTINATION_KIND})\n",
    "    WHERE (id(c1) <> id(c2))\n",
    "    WITH c1, c2, gds.similarity.cosine(c1.embedding, c2.embedding) AS cosineSimilarity\n",
    "    RETURN c2.type,c2.title,c2.content_id, cosineSimilarity\n",
    "    order by cosineSimilarity DESC\n",
    "    limit $LIMIT\n",
    "        ''', params={\n",
    "        'CONTENT_ID': content_id, \n",
    "        'ORIGIN_KIND':origin_kind, \n",
    "        'DESTINATION_KIND':destination_kind,\n",
    "        'LIMIT':limit\n",
    "        })\n",
    "    result = result.dropna()\n",
    "    return [tuple(r)[:3] for r in result.values]\n",
    "\n",
    "content_id = '34096'\n",
    "origin_kind = 'anime'\n",
    "destination_kind = 'videogame'\n",
    "get_recommendation_raw(content_id, origin_kind, destination_kind, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b8605c76-609f-427f-a073-70c37a66888d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('boardgame', 'Plunder', '12333'),\n",
       " ('boardgame', 'Fugger, Welser, Medici', '1411'),\n",
       " ('boardgame', 'Vertigo', '1315'),\n",
       " ('boardgame', 'The Russian Campaign: Southern Expansion Kit', '12052'),\n",
       " ('boardgame', 'Bali', '1352'),\n",
       " ('movie', 'Dirk Bogarde: By Myself', '375034'),\n",
       " ('movie', 'Les Dauphines', '479214'),\n",
       " ('movie', 'One Lagos Night', '831150'),\n",
       " ('movie', 'Most Beautiful Island', '438436'),\n",
       " ('movie', 'Platform', '985986'),\n",
       " ('movie', 'Choose Your Own Adventure - The Abominable Snowman', '67519'),\n",
       " ('movie', 'S.W.A.G. - Episode 2: The Texas Blues', '412872'),\n",
       " ('movie', 'Demaquillage', '1031121'),\n",
       " ('anime', 'Kishin Douji Zenki Gaiden: Anki Kitan', '5828'),\n",
       " ('anime', 'Peach Boy Riverside', '42627'),\n",
       " ('anime', 'CMFU Xueyuan: Wangzi Peng Peng Qiu', '37619'),\n",
       " ('anime', 'Wu Nao Monu', '55731'),\n",
       " ('anime', 'Tianbao Fuyao Lu', '40735'),\n",
       " ('anime', '.hack//Liminality', '299'),\n",
       " ('anime',\n",
       "  'Kouya no Kotobuki Hikoutai Gaiden: Oozora no Harukaze Hikoutai',\n",
       "  '39589'),\n",
       " ('videogame', 'Mental Hospital VR', '1530860'),\n",
       " ('videogame', 'Slurpy Derpy', '1924370'),\n",
       " ('videogame', 'Milk Bottle And Monster Girl', '1839700'),\n",
       " ('videogame', 'Ramiwo', '1215930')]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_id_from_text(text):\n",
    "    result = gds.run_cypher( '''\n",
    "    MATCH (n:content)\n",
    "    WHERE toLower(n.title) CONTAINS toLower($text)\n",
    "    RETURN n.content_id, n.type\n",
    "        ''', params={'text': text})\n",
    "    result = result.dropna()\n",
    "    return result.values[0]\n",
    "\n",
    "def get_recommendation(text):\n",
    "    id, origin_kind = get_id_from_text(text)\n",
    "    result = []\n",
    "    for destination_kind in kinds:\n",
    "        result += get_recommendation_raw(id, origin_kind, destination_kind, 8)\n",
    "    return result\n",
    "\n",
    "get_recommendation('berserk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e4fea24e-6f31-4a8f-afcf-fe992f2748e2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[112], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mstop\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b232c5e7-0828-4e0c-8ba5-e5973cf5bdc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72953abc-d09c-4d65-98d6-ea6a2872fdd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "64827382-a75d-43f0-9579-7bc5e9fc1853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gds.graph.drop('embedding-projection')\n",
    "# gds.graph.drop('embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "8f8e104c-ecf9-4dcc-809e-0857a7e1d43f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gds.run_cypher('CREATE INDEX content_id FOR (n:content) ON (n.content_id)')\n",
    "# gds.run_cypher('CREATE INDEX keywords FOR (n:keyword) ON (n.keyword)')\n",
    "# gds.run_cypher('CREATE INDEX users_id FOR (n:users) ON (n.users_id)')\n",
    "# gds.run_cypher('CREATE INDEX content_type FOR (n:content) ON (n.type)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6fd0f2-b2a1-4612-84e0-03ce9b576d04",
   "metadata": {},
   "source": [
    "## Making some prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803e6f68-0df8-42c6-9e00-9194f5bb0aec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "46dcc978-f402-4268-bf5b-0d8be7adedf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nodeProjection            {'content': {'label': 'content', 'properties':...\n",
       "relationshipProjection    {'__ALL__': {'aggregation': 'DEFAULT', 'orient...\n",
       "graphName                                                     cf-projection\n",
       "nodeCount                                                              3788\n",
       "relationshipCount                                                         0\n",
       "projectMillis                                                            50\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# g1, res = gds.graph.project('cf-projection', {'content':{'properties':['embedding']}},'*')\n",
    "# res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3daba865-01c6-4787-bcaa-3de7cf73a452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e322e5d32e74818904b503bd3e37e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Knn:   0%|          | 0/100 [00:00<?, ?%/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# knn_stats_df = gds.knn.write(g1, nodeProperties=['embedding'],\n",
    "#     writeRelationshipType='USERS_ALSO_LIKED',\n",
    "#     writeProperty='score',\n",
    "#     sampleRate=1.0,\n",
    "#     maxIterations=1000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a373494f-f485-4eea-b52c-72e97824a59b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "didConverge                True\n",
       "ranIterations                59\n",
       "computeMillis               568\n",
       "writeMillis                 347\n",
       "nodesCompared              3788\n",
       "nodePairsConsidered     5048902\n",
       "relationshipsWritten      37880\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# knn_stats_df[['didConverge',\n",
    "#               'ranIterations',\n",
    "#               'computeMillis',\n",
    "#               'writeMillis',\n",
    "#               'nodesCompared',\n",
    "#               'nodePairsConsidered',\n",
    "#               'relationshipsWritten']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "01ad02cb-b1d0-43ab-a6b6-d12d4d62e811",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CONTENT_ID = '128'\n",
    "# gds.run_cypher( '''\n",
    "#     MATCH (n:content {content_id: $CONTENT_ID})-[r:USERS_ALSO_LIKED]->(o:content)\n",
    "#     return distinct o.title,o.content_id,sum(r.score) AS score, o.type\n",
    "#     ORDER BY score DESC\n",
    "#     ''', params={'CONTENT_ID': CONTENT_ID})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b73e97b0-7a45-4d1f-9880-ab77e7fb3700",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting neo4j\n",
      "  Using cached neo4j-5.20.0-py3-none-any.whl\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.11/site-packages (from neo4j) (2023.3)\n",
      "Installing collected packages: neo4j\n",
      "Successfully installed neo4j-5.20.0\n"
     ]
    }
   ],
   "source": [
    "!pip install neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ea49b2c8-d7ab-4081-a922-d8e3a874b96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4804437d-ddea-47ef-97b6-5ce098178edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "DRIVER = neo4j.GraphDatabase.driver(uri=\"neo4j://neo4j\")\n",
    "\n",
    "\n",
    "def execute_cypher(query: str):\n",
    "    \"\"\"\n",
    "    Executes a Cypher @query and returns its result.\n",
    "    \"\"\"\n",
    "    result = DRIVER.execute_query(query)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6336f0b9-8017-4949-870f-36833b80a861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_keywords_recommender(content_id, kind):\n",
    "    '''\n",
    "    Gets the common keywords from one content \n",
    "\n",
    "    Example \n",
    "    - content_id = 1210\n",
    "    - type = boardgame\n",
    "    '''\n",
    "    \n",
    "    return f'''\n",
    "        MATCH (c:content) -[]->(k:keyword)<-[]-(other:content)\n",
    "        where c.content_id = \"{content_id}\" and c.type = \"{kind}\"\n",
    "        WITH other, count(k) AS sharedKeywords\n",
    "        ORDER BY sharedKeywords DESC\n",
    "        RETURN other.content_id,other.type, sharedKeywords\n",
    "        LIMIT 5\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7ce3c32a-d8e5-49aa-b4e6-e38a2c8424c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        MATCH (c:content) -[]->(k:keyword)<-[]-(other:content)\n",
      "        where c.content_id = \"1210\" and c.type = \"boardgame\"\n",
      "        WITH other, count(k) AS sharedKeywords\n",
      "        ORDER BY sharedKeywords DESC\n",
      "        RETURN other.content_id,other.type, sharedKeywords\n",
      "        LIMIT 5\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "a= get_common_keywords_recommender('1210','boardgame')\n",
    "print (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0ae0426e-df19-4107-8580-259584e7edb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+------------+---------+--------------------+----------+\n",
      "|          keywords|         description|release_year|     type|               title|content_id|\n",
      "+------------------+--------------------+------------+---------+--------------------+----------+\n",
      "|     united states|In Cults Across A...|        1998|boardgame|Cults Across America|      1210|\n",
      "|     entire united|In Cults Across A...|        1998|boardgame|Cults Across America|      1210|\n",
      "|           america|In Cults Across A...|        1998|boardgame|Cults Across America|      1210|\n",
      "|             cults|In Cults Across A...|        1998|boardgame|Cults Across America|      1210|\n",
      "|     standard game|In Cults Across A...|        1998|boardgame|Cults Across America|      1210|\n",
      "|              game|In Cults Across A...|        1998|boardgame|Cults Across America|      1210|\n",
      "|            united|In Cults Across A...|        1998|boardgame|Cults Across America|      1210|\n",
      "|            states|In Cults Across A...|        1998|boardgame|Cults Across America|      1210|\n",
      "|religious fanatics|In Cults Across A...|        1998|boardgame|Cults Across America|      1210|\n",
      "|          standard|In Cults Across A...|        1998|boardgame|Cults Across America|      1210|\n",
      "|           players|In Cults Across A...|        1998|boardgame|Cults Across America|      1210|\n",
      "|        play cults|In Cults Across A...|        1998|boardgame|Cults Across America|      1210|\n",
      "|             coast|In Cults Across A...|        1998|boardgame|Cults Across America|      1210|\n",
      "|    strategic game|In Cults Across A...|        1998|boardgame|Cults Across America|      1210|\n",
      "|               aim|In Cults Across A...|        1998|boardgame|Cults Across America|      1210|\n",
      "+------------------+--------------------+------------+---------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kw_df.filter(col('content_id') == '1210').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c20d3c-6e86-4e5e-99a9-c1c5a51227e3",
   "metadata": {},
   "source": [
    "# Random Things "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "28924f47-01bc-48fd-a2aa-7bfc3808745e",
   "metadata": {},
   "outputs": [],
   "source": [
    "type = 'Boardgame'\n",
    "(\n",
    "    merged_content\n",
    "    .filter(merged_content['type'] == str.lower(type))\n",
    "    .write\n",
    "    .format(\"org.neo4j.spark.DataSource\")\n",
    "    .mode(\"Append\")\n",
    "    .option(\"labels\", f\":{type}\")\n",
    "    .option(\"url\", NEO4J_URL)\n",
    "    .save()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bff002b-aee7-4861-baba-5bd244df68b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d692155-e59d-4e15-875c-e0b850299042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = (\n",
    "#     SparkSession\n",
    "#     .builder\n",
    "#     .appName(\"Neo4j-Spark Connector\")\n",
    "#     # .config(\"spark.jars.packages\", \"neo4j-contrib:neo4j-spark-connector:5.3.0_for_spark_3\")\n",
    "#     .config(\"spark.jars.packages\", \"org.neo4j:neo4j-connector-apache-spark_2.12:5.3.0_for_spark_3\")\n",
    "#     # $SPARK_HOME/bin/pyspark --packages org.neo4j:neo4j-connector-apache-spark_2.12:5.3.0_for_spark_3\n",
    "#     .config(\"spark.neo4j.bolt.url\", NEO4J_URL)\n",
    "#     # .config(\"spark.neo4j.bolt.url\", \"bolt://neo4j\")\n",
    "#     # .config(\"spark.neo4j.bolt.user\", \"neo4j\")\n",
    "#     # .config(\"spark.neo4j.bolt.password\", \"password\")\n",
    "#     .getOrCreate()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dc237fae-853e-482c-8b6c-9415bc6900b8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nodes_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnodes_df\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nodes_df' is not defined"
     ]
    }
   ],
   "source": [
    "nodes_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0a61b62f-9708-4b84-999f-576064680cd9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o708.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 62.0 failed 1 times, most recent failure: Lost task 15.0 in stage 62.0 (TID 551) (c730d8c7acf6 executor driver): java.lang.IllegalArgumentException: Write failed due to the following errors:\n - Schema is missing id from option `relationship.target.node.keys`\n - Schema is missing id from option `relationship.source.node.keys`\n\nThe option key and value might be inverted.\n\tat org.neo4j.spark.util.ValidateSchemaOptions.validate(Validations.scala:41)\n\tat org.neo4j.spark.util.Validations$.$anonfun$validate$1(Validations.scala:15)\n\tat org.neo4j.spark.util.Validations$.$anonfun$validate$1$adapted(Validations.scala:15)\n\tat scala.collection.immutable.Set$Set1.foreach(Set.scala:141)\n\tat org.neo4j.spark.util.Validations$.validate(Validations.scala:15)\n\tat org.neo4j.spark.service.Neo4jWriteMappingStrategy.relationship(MappingService.scala:100)\n\tat org.neo4j.spark.service.Neo4jWriteMappingStrategy.relationship(MappingService.scala:21)\n\tat org.neo4j.spark.service.MappingService.convert(MappingService.scala:246)\n\tat org.neo4j.spark.writer.BaseDataWriter.write(BaseDataWriter.scala:46)\n\tat org.neo4j.spark.writer.BaseDataWriter.write(BaseDataWriter.scala:22)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.write(WriteToDataSourceV2Exec.scala:516)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:471)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:509)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:448)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:514)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:411)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:408)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:382)\n\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExec.writeWithV2(WriteToDataSourceV2Exec.scala:248)\n\tat org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run(WriteToDataSourceV2Exec.scala:360)\n\tat org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run$(WriteToDataSourceV2Exec.scala:359)\n\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExec.run(WriteToDataSourceV2Exec.scala:248)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:311)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.lang.IllegalArgumentException: Write failed due to the following errors:\n - Schema is missing id from option `relationship.target.node.keys`\n - Schema is missing id from option `relationship.source.node.keys`\n\nThe option key and value might be inverted.\n\tat org.neo4j.spark.util.ValidateSchemaOptions.validate(Validations.scala:41)\n\tat org.neo4j.spark.util.Validations$.$anonfun$validate$1(Validations.scala:15)\n\tat org.neo4j.spark.util.Validations$.$anonfun$validate$1$adapted(Validations.scala:15)\n\tat scala.collection.immutable.Set$Set1.foreach(Set.scala:141)\n\tat org.neo4j.spark.util.Validations$.validate(Validations.scala:15)\n\tat org.neo4j.spark.service.Neo4jWriteMappingStrategy.relationship(MappingService.scala:100)\n\tat org.neo4j.spark.service.Neo4jWriteMappingStrategy.relationship(MappingService.scala:21)\n\tat org.neo4j.spark.service.MappingService.convert(MappingService.scala:246)\n\tat org.neo4j.spark.writer.BaseDataWriter.write(BaseDataWriter.scala:46)\n\tat org.neo4j.spark.writer.BaseDataWriter.write(BaseDataWriter.scala:22)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.write(WriteToDataSourceV2Exec.scala:516)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:471)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:509)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:448)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:514)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:411)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 37\u001b[0m\n\u001b[1;32m     22\u001b[0m relationships_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame([\n\u001b[1;32m     23\u001b[0m     Row(src\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, dst\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, relationship\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFRIENDS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m ])\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Write relationships to Neo4j\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[43mrelationships_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.neo4j.spark.DataSource\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAppend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrelationship\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFRIENDS\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrelationship.save.strategy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkeys\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrelationship.source.labels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m:Person\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrelationship.source.node.keys\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid:src\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrelationship.target.labels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m:Person\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrelationship.target.node.keys\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid:dst\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNEO4J_URL\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 37\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/readwriter.py:1461\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1459\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1461\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1463\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave(path)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o708.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 62.0 failed 1 times, most recent failure: Lost task 15.0 in stage 62.0 (TID 551) (c730d8c7acf6 executor driver): java.lang.IllegalArgumentException: Write failed due to the following errors:\n - Schema is missing id from option `relationship.target.node.keys`\n - Schema is missing id from option `relationship.source.node.keys`\n\nThe option key and value might be inverted.\n\tat org.neo4j.spark.util.ValidateSchemaOptions.validate(Validations.scala:41)\n\tat org.neo4j.spark.util.Validations$.$anonfun$validate$1(Validations.scala:15)\n\tat org.neo4j.spark.util.Validations$.$anonfun$validate$1$adapted(Validations.scala:15)\n\tat scala.collection.immutable.Set$Set1.foreach(Set.scala:141)\n\tat org.neo4j.spark.util.Validations$.validate(Validations.scala:15)\n\tat org.neo4j.spark.service.Neo4jWriteMappingStrategy.relationship(MappingService.scala:100)\n\tat org.neo4j.spark.service.Neo4jWriteMappingStrategy.relationship(MappingService.scala:21)\n\tat org.neo4j.spark.service.MappingService.convert(MappingService.scala:246)\n\tat org.neo4j.spark.writer.BaseDataWriter.write(BaseDataWriter.scala:46)\n\tat org.neo4j.spark.writer.BaseDataWriter.write(BaseDataWriter.scala:22)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.write(WriteToDataSourceV2Exec.scala:516)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:471)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:509)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:448)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:514)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:411)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:408)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:382)\n\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExec.writeWithV2(WriteToDataSourceV2Exec.scala:248)\n\tat org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run(WriteToDataSourceV2Exec.scala:360)\n\tat org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run$(WriteToDataSourceV2Exec.scala:359)\n\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExec.run(WriteToDataSourceV2Exec.scala:248)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:311)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.lang.IllegalArgumentException: Write failed due to the following errors:\n - Schema is missing id from option `relationship.target.node.keys`\n - Schema is missing id from option `relationship.source.node.keys`\n\nThe option key and value might be inverted.\n\tat org.neo4j.spark.util.ValidateSchemaOptions.validate(Validations.scala:41)\n\tat org.neo4j.spark.util.Validations$.$anonfun$validate$1(Validations.scala:15)\n\tat org.neo4j.spark.util.Validations$.$anonfun$validate$1$adapted(Validations.scala:15)\n\tat scala.collection.immutable.Set$Set1.foreach(Set.scala:141)\n\tat org.neo4j.spark.util.Validations$.validate(Validations.scala:15)\n\tat org.neo4j.spark.service.Neo4jWriteMappingStrategy.relationship(MappingService.scala:100)\n\tat org.neo4j.spark.service.Neo4jWriteMappingStrategy.relationship(MappingService.scala:21)\n\tat org.neo4j.spark.service.MappingService.convert(MappingService.scala:246)\n\tat org.neo4j.spark.writer.BaseDataWriter.write(BaseDataWriter.scala:46)\n\tat org.neo4j.spark.writer.BaseDataWriter.write(BaseDataWriter.scala:22)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.write(WriteToDataSourceV2Exec.scala:516)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:471)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1563)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:509)\n\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:448)\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:514)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:411)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "nodes_df = spark.createDataFrame([\n",
    "    Row(id=1, name=\"Alicee\"),\n",
    "    Row(id=2, name=\"Bob\")\n",
    "])\n",
    "\n",
    "# relationships_df = spark.createDataFrame([\n",
    "#     Row(src=1, dst=2, relationship=\"KNOWS\")\n",
    "# ])\n",
    "\n",
    "# Write nodes to Neo4j\n",
    "(\n",
    "    nodes_df\n",
    "    .write\n",
    "    .format(\"org.neo4j.spark.DataSource\")\n",
    "    .mode(\"Overwrite\")\n",
    "    .mode(\"Append\")\n",
    "    .option(\"labels\", \":Person\")\n",
    "    .option(\"url\", NEO4J_URL)\n",
    "    .save()\n",
    ")\n",
    "\n",
    "relationships_df = spark.createDataFrame([\n",
    "    Row(src=1, dst=2, relationship=\"FRIENDS\")\n",
    "])\n",
    "\n",
    "# Write relationships to Neo4j\n",
    "relationships_df.write \\\n",
    "    .format(\"org.neo4j.spark.DataSource\") \\\n",
    "    .mode(\"Append\") \\\n",
    "    .option(\"relationship\", \"FRIENDS\") \\\n",
    "    .option(\"relationship.save.strategy\", \"keys\") \\\n",
    "    .option(\"relationship.source.labels\", \":Person\") \\\n",
    "    .option(\"relationship.source.node.keys\", \"id:src\") \\\n",
    "    .option(\"relationship.target.labels\", \":Person\") \\\n",
    "    .option(\"relationship.target.node.keys\", \"id:dst\") \\\n",
    "    .option(\"url\", NEO4J_URL) \\\n",
    "    .save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cfe21528-5dd6-4e96-9e2b-221f4e19b79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c0696b97-b3da-4a19-a081-128228db1611",
   "metadata": {},
   "outputs": [],
   "source": [
    "DRIVER = neo4j.GraphDatabase.driver(uri=\"neo4j://neo4j\")\n",
    "\n",
    "\n",
    "def execute(query: str):\n",
    "    \"\"\"\n",
    "    Executes a Cypher @query and returns its result.\n",
    "    \"\"\"\n",
    "    result = DRIVER.execute_query(query)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "eef24fd9-c588-4af7-8b0e-3c26599d02f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EagerResult(records=[], summary=<neo4j._work.summary.ResultSummary object at 0x7faba574bc10>, keys=['r', 'n1', 'n2'])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execute('MATCH (n1)-[r]->(n2) RETURN r, n1, n2 LIMIT 25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc36d059-6126-4c3c-864d-9b5c2e77388e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "import neo4j\n",
    "import yake\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "SEED = 13\n",
    "# SEMANTIC_PATH = '../semanticscholar_raw_data'\n",
    "SEMANTIC_PATH = '../small_sample'\n",
    "DEFAULT_JOURNAL_NAME = 'Unknown'\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DRIVER = neo4j.GraphDatabase.driver(uri=\"neo4j://localhost\")\n",
    "\n",
    "\n",
    "def execute(query: str):\n",
    "    \"\"\"\n",
    "    Executes a Cypher @query and returns its result.\n",
    "    \"\"\"\n",
    "    result = DRIVER.execute_query(query)\n",
    "    return result\n",
    "\n",
    "\n",
    "def delete_graph() -> None:\n",
    "    \"\"\"\n",
    "    Deletes every node and edge of the graph.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "        MATCH (n)\n",
    "        DETACH DELETE n;\n",
    "    \"\"\"\n",
    "\n",
    "    execute(query)\n",
    "\n",
    "\n",
    "def parse_journal_name(paper) -> str:\n",
    "    \"\"\"\n",
    "    Not every file has a field 'journal' in the json.\n",
    "    This function treats those edge cases.\n",
    "    \"\"\"\n",
    "    if 'journal' not in paper or not paper['journal']:\n",
    "        return DEFAULT_JOURNAL_NAME\n",
    "    else:\n",
    "        return paper.get('journal', {'name': DEFAULT_JOURNAL_NAME}).get('name', DEFAULT_JOURNAL_NAME).replace(\"'\", '').replace('\"', '')\n",
    "\n",
    "\n",
    "def sanitize_abstract(abstract: str) -> str:\n",
    "    if abstract:\n",
    "        return (\n",
    "            abstract\n",
    "            .replace('\"', \"'\")\n",
    "            .replace('\\\\', '\\\\\\\\')\n",
    "        )\n",
    "    else:\n",
    "        return abstract\n",
    "\n",
    "\n",
    "def create_papers():\n",
    "    \"\"\"\n",
    "    Create the nodes of label `Paper`.\n",
    "    \"\"\"\n",
    "    # This is used to extract the keywords from the abstract.\n",
    "    kw_extractor = yake.KeywordExtractor(\n",
    "        lan='en',\n",
    "        n=3,  # Max n-gram size\n",
    "        top=5  # Number of keywords\n",
    "    )\n",
    "\n",
    "    for fname in tqdm(os.listdir(SEMANTIC_PATH)):\n",
    "        with open(f'{SEMANTIC_PATH}/{fname}') as f:\n",
    "            paper = json.loads(f.read())\n",
    "\n",
    "        title = paper['title'].replace('\\\\', '').replace('\"', \"'\")\n",
    "        keywords = kw_extractor.extract_keywords(paper['abstract']) if paper['abstract'] else ''\n",
    "        keywords = list(map(lambda x: str.lower(x[0]) if x else '', keywords))\n",
    "\n",
    "        # publication_venue: \"{paper['publicationVenue']}\",\n",
    "        # venue: \"{paper['venue']}\",\n",
    "        # fieldsOfStudy: {paper['fieldsOfStudy'] if paper['fieldsOfStudy'] else '[]'},\n",
    "        query = f\"\"\"\n",
    "        CREATE (n:Paper {{\n",
    "            paper_id: \"{paper['paperId']}\",\n",
    "            title: \"{title}\",\n",
    "            year: toInteger({paper['year'] if paper['year'] else -1}),\n",
    "            publicationDate: date(\"{paper['publicationDate'] if paper['publicationDate'] else '1970-01-01'}\"),\n",
    "            abstract: \"{sanitize_abstract(paper['abstract'])}\",\n",
    "            keywords: {keywords}\n",
    "        }})\n",
    "        \"\"\"\n",
    "        try:\n",
    "            execute(query)\n",
    "        except:\n",
    "            print(query)\n",
    "\n",
    "\n",
    "def create_paper__paper_id__range_index():\n",
    "    \"\"\"\n",
    "    Create indexes\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "        CREATE RANGE INDEX paper__paper_id__range_index IF NOT EXISTS\n",
    "        FOR (n:Paper)\n",
    "        ON (n.paper_id)\n",
    "    \"\"\"\n",
    "\n",
    "    execute(query)\n",
    "\n",
    "\n",
    "def create_author__author_id__range_index():\n",
    "    \"\"\"\n",
    "    Create indexes\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "        CREATE RANGE INDEX author__author_id__range_index IF NOT EXISTS\n",
    "        FOR (n:Author)\n",
    "        ON (n.author_id)\n",
    "    \"\"\"\n",
    "\n",
    "    execute(query)\n",
    "\n",
    "\n",
    "def create_authors() -> None:\n",
    "    \"\"\"\n",
    "    For each paper, generate a node with label `Author` for that paper.\n",
    "    We are using the MERGE here since we don't want to duplicate authors.\n",
    "    \"\"\"\n",
    "    for fname in tqdm(os.listdir(SEMANTIC_PATH)):\n",
    "        # print(f'Creating the authors of {fname}')\n",
    "\n",
    "        with open(f'{SEMANTIC_PATH}/{fname}') as f:\n",
    "            paper = json.loads(f.read())\n",
    "\n",
    "        for author in paper['authors']:\n",
    "            query = f\"\"\"\n",
    "            MERGE (n:Author {{\n",
    "                name: \"{author['name']}\",\n",
    "                author_id: \"{author['authorId']}\"\n",
    "            }})\n",
    "            \"\"\"\n",
    "            execute(query)\n",
    "\n",
    "\n",
    "def link_author_to_paper() -> None:\n",
    "    \"\"\"\n",
    "    Create the edge `Wrote` and `IsCorrespondingAuthor`, linking Authors and Papers.\n",
    "    The first author is considered the corresponding author.\n",
    "    \"\"\"\n",
    "    for fname in tqdm(os.listdir(SEMANTIC_PATH)):\n",
    "        # print(f'Linking authors of file {fname}')\n",
    "\n",
    "        with open(f'{SEMANTIC_PATH}/{fname}') as f:\n",
    "            paper = json.loads(f.read())\n",
    "\n",
    "            is_first = True\n",
    "            for author in paper['authors']:\n",
    "                if is_first:\n",
    "                    # The first author is the main corresponding author.\n",
    "                    query = f\"\"\"\n",
    "                        MATCH (a:Author {{author_id: '{author['authorId']}'}})\n",
    "                        WITH a\n",
    "                        MATCH (p:Paper {{paper_id: '{paper['paperId']}'}})\n",
    "                        WITH a, p\n",
    "                        CREATE (a)-[e:IsCorrespondingAuthor]->(p);\n",
    "                    \"\"\"\n",
    "                    execute(query)\n",
    "                    is_first = False\n",
    "\n",
    "                query = f\"\"\"\n",
    "                    MATCH (a:Author {{author_id: '{author['authorId']}'}})\n",
    "                    WITH a\n",
    "                    MATCH (p:Paper {{paper_id: '{paper['paperId']}'}})\n",
    "                    WITH a, p\n",
    "                    CREATE (a)-[e:Wrote]->(p);\n",
    "                \"\"\"\n",
    "\n",
    "                execute(query)\n",
    "\n",
    "\n",
    "def link_citations_between_papers() -> None:\n",
    "    \"\"\"\n",
    "    Generate the edge Cited linking a Paper to a Paper.\n",
    "    \"\"\"\n",
    "    for fname in tqdm(os.listdir(SEMANTIC_PATH)):\n",
    "        # print(f'Linking citations of file {fname}')\n",
    "\n",
    "        with open(f'{SEMANTIC_PATH}/{fname}') as f:\n",
    "            paper = json.loads(f.read())\n",
    "\n",
    "        for citation in paper.get('citations', []):\n",
    "            query = f\"\"\"\n",
    "                MATCH (a:Paper {{paper_id: '{citation['paperId']}'}})\n",
    "                WITH a\n",
    "                MATCH (p:Paper {{paper_id: '{paper['paperId']}'}})\n",
    "                CREATE (a)-[e:Cites]->(p);\n",
    "            \"\"\"\n",
    "            execute(query)\n",
    "\n",
    "\n",
    "def create_journals() -> None:\n",
    "    \"\"\"\n",
    "    Create the Journal nodes.\n",
    "    \"\"\"\n",
    "    for fname in tqdm(os.listdir(SEMANTIC_PATH)):\n",
    "        with open(f'{SEMANTIC_PATH}/{fname}') as f:\n",
    "            paper = json.loads(f.read())\n",
    "\n",
    "        journal_name = parse_journal_name(paper)\n",
    "\n",
    "        if journal_name != DEFAULT_JOURNAL_NAME:\n",
    "            query = f\"\"\"\n",
    "                MERGE (n:Journal {{\n",
    "                    year: toInteger({paper['year'] if paper['year'] else -1}),\n",
    "                    name: \"{journal_name}\"\n",
    "                }})\n",
    "            \"\"\"\n",
    "\n",
    "            execute(query)\n",
    "\n",
    "\n",
    "def link_journals()-> None:\n",
    "    \"\"\"\n",
    "    Link a Paper to a Journal creating the `PublishedIn` edge.\n",
    "    \"\"\"\n",
    "    for fname in tqdm(os.listdir(SEMANTIC_PATH)):\n",
    "        with open(f'{SEMANTIC_PATH}/{fname}') as f:\n",
    "            paper = json.loads(f.read())\n",
    "\n",
    "        query = f\"\"\"\n",
    "            MATCH (p:Paper {{paper_id: '{paper['paperId']}'}})\n",
    "            WITH p\n",
    "            MATCH (j:Journal {{name: '{parse_journal_name(paper)}', year: toInteger({paper['year'] if paper['year'] else -1})}})\n",
    "            WITH p, j\n",
    "            CREATE (p)-[e:PublishedIn]->(j);\n",
    "        \"\"\"\n",
    "        execute(query)\n",
    "\n",
    "\n",
    "def change_to_conference() -> None:\n",
    "    \"\"\"\n",
    "    Change the label from Journal to Conference if the \"Journal\" name contains 'conference' in it.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "        MATCH (j:Journal)\n",
    "        WHERE toLower(j.name) =~ '.*conference.*'\n",
    "           OR toLower(j.name) =~ '.*workshop.*'\n",
    "           OR toLower(j.name) =~ '.*proc\\..*'\n",
    "        REMOVE j:Journal\n",
    "        SET j:ConfWork\n",
    "    \"\"\"\n",
    "\n",
    "    execute(query)\n",
    "\n",
    "\n",
    "def get_possible_reviewers():\n",
    "    \"\"\"\n",
    "    Auxiliary function that returns an aggregation of all possible reviewers of a paper.\n",
    "    The logic of a \"possible reviewer\" is to select an author who:\n",
    "    1. wrote paper(s) cited by the paper in question; and who\n",
    "    2. didn't wrote the paper itself.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "        MATCH (a:Author)-[w1:Wrote]->(mp:Paper)-[c:Cites]->(cp:Paper)\n",
    "        WITH mp, cp, a\n",
    "        MATCH (wcp:Author)-[w2:Wrote]->(cp)\n",
    "        WHERE NOT (wcp)-[:Wrote]->(mp)\n",
    "        RETURN mp.paper_id AS paper_id, collect(wcp.author_id) AS possible_reviewer_ids;\n",
    "    \"\"\"\n",
    "\n",
    "    return execute(query)\n",
    "\n",
    "\n",
    "def link_reviewer_to_paper() -> None:\n",
    "    \"\"\"\n",
    "    This function generates synthetic data.\n",
    "    \"\"\"\n",
    "    result = get_possible_reviewers()\n",
    "\n",
    "    for paper_id, possible_reviewers in tqdm(result[0]):\n",
    "        # Papers can have a different amount of reviewers, varying from 1 to 4, following the distribution specified by `p`.\n",
    "        # Edge case: If the paper doesn't cite any other paper, it will have 0 reviewers.\n",
    "        reviewer_qty = min(\n",
    "            np.random.choice(np.arange(1, 5), p=[0.1, 0.3, 0.5, 0.1]),\n",
    "            len(possible_reviewers)\n",
    "        )\n",
    "\n",
    "        reviewers = random.sample(possible_reviewers, reviewer_qty)\n",
    "        for reviewer in reviewers:\n",
    "            query = f\"\"\"\n",
    "                MATCH (a:Author {{author_id: '{reviewer}'}})\n",
    "                WITH a\n",
    "                MATCH (p:Paper {{paper_id: '{paper_id}'}})\n",
    "                CREATE (a)-[e:Reviewed]->(p);\n",
    "            \"\"\"\n",
    "\n",
    "            execute(query)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"Deleting graph\")\n",
    "    delete_graph()\n",
    "    print(\"Papers\")\n",
    "    create_papers()\n",
    "    create_paper__paper_id__range_index()\n",
    "    print(\"Authors\")\n",
    "    create_author__author_id__range_index()\n",
    "    create_authors()\n",
    "    print(\"Wrote\")\n",
    "    link_author_to_paper()\n",
    "    print(\"Citations\")\n",
    "    link_citations_between_papers()\n",
    "    print(\"Journals/Conferences\")\n",
    "    create_journals()\n",
    "    print(\"Linking journals\")\n",
    "    link_journals()\n",
    "    change_to_conference()\n",
    "    print(\"Reviewers\")\n",
    "    link_reviewer_to_paper()\n",
    "\n",
    "\n",
    "    print(\"Querying...\")\n",
    "\n",
    "    print(\"Query 1\")\n",
    "    execute(\"\"\"\n",
    "MATCH (p:Paper)-[:cited]->(cited:Paper) WITH p.name AS journal, p.title AS title, COUNT(*) AS num_citations ORDER BY journal, num_citations DESC WITH journal, COLLECT({title: title, num_citations: num_citations}) AS papers WITH journal, papers, [i IN RANGE(1, SIZE(papers)) | i] AS ranks UNWIND ranks AS rank WITH journal, papers[rank - 1].title AS title, papers[rank - 1].num_citations AS num_citations, rank WHERE rank <= 3 RETURN journal, title, num_citations, rank ORDER BY journal, rank\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"Query 2\")\n",
    "    execute(\"\"\"\n",
    "MATCH (a:Author)-[:Wrote]->(p:Paper)-[:PublishedIn]->(c:ConfWork)\n",
    "WITH a.name AS author, collect(DISTINCT c.year) AS years, c.name AS conference\n",
    "WHERE size(years) > 3\n",
    "RETURN author, years, conference\n",
    "ORDER BY author, conference\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"Query 3\")\n",
    "    execute(\"\"\"\n",
    "MATCH (citing_paper:Paper)-[:Cites]->(published_paper:Paper {year: j.year})-[:PublishedIn]->(j:Journal)\n",
    "WITH COUNT(DISTINCT citing_paper) AS total_citations, j.name AS journal_name, j AS j1\n",
    "MATCH (j2: Journal)<-[:PublishedIn]-(p:Paper)\n",
    "WHERE j2.year IN [j1.year - 1, j1.year - 2]\n",
    "      AND j1.name = j2.name\n",
    "WITH j1.year AS year,\n",
    "     COUNT(p.title) AS past_publications,\n",
    "     j1.name AS journal_name,\n",
    "     total_citations\n",
    "RETURN year, journal_name, total_citations, past_publications, 1.0 * total_citations / past_publications\n",
    "ORDER BY journal_name, year;\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"Query 4\")\n",
    "    execute(\"\"\"\n",
    "MATCH (a:Author)-[:Wrote]->(p:Paper)-[:cited]->(cited:Paper) WITH a, p, COUNT(*) AS num_citations ORDER BY num_citations DESC WITH a, COLLECT(num_citations) AS citation_counts WITH a, [i IN RANGE(1, SIZE(citation_counts)) | CASE WHEN citation_counts[i - 1] >= i THEN i ELSE 0 END] AS h_values WITH a, MAX(h_values) AS h_index WITH a, MAX(REDUCE(s = 0, h IN h_index | CASE WHEN h > s THEN h ELSE s END)) AS max_h_index RETURN a.author_id AS author_id, a.name AS author_name, max_h_index\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"Recommendation system\")\n",
    "    print(\"Part 1\")\n",
    "    execute(\"\"\"\n",
    "// First we are looking for papers containing any of those keywords.\n",
    "MATCH (p:Paper)\n",
    "WHERE\n",
    "    // Could've been an array intersection, but APOC was giving us some setup issues.\n",
    "    'data management' IN p.keywords\n",
    "    OR 'indexing' IN p.keywords\n",
    "    OR 'data modeling' IN p.keywords\n",
    "    OR 'big data' IN p.keywords\n",
    "    OR 'data processing' IN p.keywords\n",
    "    OR 'data storage' IN p.keywords\n",
    "    OR 'data querying' IN p.keywords\n",
    "RETURN *\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"Part 2\")\n",
    "    execute(\"\"\"\n",
    "// Now we want the conferences or journals with at least 90% of published papers being related to databases.\n",
    "MATCH (p:Paper)-[:PublishedIn]->(jc)\n",
    "WITH p, (\n",
    "        'data management' IN p.keywords\n",
    "        OR 'indexing' IN p.keywords\n",
    "        OR 'data modeling' IN p.keywords\n",
    "        OR 'big data' IN p.keywords\n",
    "        OR 'data processing' IN p.keywords\n",
    "        OR 'data storage' IN p.keywords\n",
    "        OR 'data querying' IN p.keywords\n",
    "    ) AS in_db_community,\n",
    "    jc\n",
    "WITH COUNT(p) AS total_published_papers, SUM(CASE in_db_community WHEN TRUE THEN 1 ELSE 0 END) AS db_comm_papers, jc.name AS jc_name\n",
    "WHERE 100.0 * db_comm_papers / total_published_papers > 90.0 \n",
    "RETURN total_published_papers, db_comm_papers, 100.0 * db_comm_papers / total_published_papers AS percentage_of_db_papers, jc_name\n",
    "LIMIT 50\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"Part 3\")\n",
    "    execute(\"\"\"\n",
    "// Let's now grab the top 100 most cited papers in the Database community.\n",
    "MATCH (p:Paper)-[:PublishedIn]->(jc)\n",
    "WITH p, (\n",
    "        'data management' IN p.keywords\n",
    "        OR 'indexing' IN p.keywords\n",
    "        OR 'data modeling' IN p.keywords\n",
    "        OR 'big data' IN p.keywords\n",
    "        OR 'data processing' IN p.keywords\n",
    "        OR 'data storage' IN p.keywords\n",
    "        OR 'data querying' IN p.keywords\n",
    "    ) AS in_db_community,\n",
    "    jc\n",
    "WITH COUNT(p) AS total_published_papers, SUM(CASE in_db_community WHEN TRUE THEN 1 ELSE 0 END) AS db_comm_papers, jc.name AS jc_name, jc\n",
    "WHERE 100.0 * db_comm_papers / total_published_papers > 90.0\n",
    "WITH collect(jc.name) AS db_comm_conferences\n",
    "\n",
    "MATCH (citing_paper:Paper)-[:Cites]->(cited_paper:Paper)-[:PublishedIn]->(jc1), (citing_paper)-[:PublishedIn]->(jc2)\n",
    "WHERE jc1.name IN db_comm_conferences\n",
    "  AND jc2.name IN db_comm_conferences\n",
    "WITH cited_paper, jc1, COUNT(DISTINCT citing_paper) AS c\n",
    "RETURN c, jc1.name, cited_paper.title\n",
    "ORDER BY c DESC\n",
    "LIMIT 100\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"Part 4\")\n",
    "    execute(\"\"\"\n",
    "// Now, we will find the gurus of the community.\n",
    "MATCH (p:Paper)-[:PublishedIn]->(jc)\n",
    "WITH p, (\n",
    "        'data management' IN p.keywords\n",
    "        OR 'indexing' IN p.keywords\n",
    "        OR 'data modeling' IN p.keywords\n",
    "        OR 'big data' IN p.keywords\n",
    "        OR 'data processing' IN p.keywords\n",
    "        OR 'data storage' IN p.keywords\n",
    "        OR 'data querying' IN p.keywords\n",
    "    ) AS in_db_community,\n",
    "    jc\n",
    "WITH COUNT(p) AS total_published_papers, SUM(CASE in_db_community WHEN TRUE THEN 1 ELSE 0 END) AS db_comm_papers, jc.name AS jc_name, jc\n",
    "WHERE 100.0 * db_comm_papers / total_published_papers > 90.0\n",
    "WITH collect(jc.name) AS db_comm_conferences\n",
    "\n",
    "MATCH (citing_paper:Paper)-[:Cites]->(cited_paper:Paper)-[:PublishedIn]->(jc1), (citing_paper)-[:PublishedIn]->(jc2)\n",
    "WHERE jc1.name IN db_comm_conferences\n",
    "  AND jc2.name IN db_comm_conferences\n",
    "WITH cited_paper, jc1, COUNT(DISTINCT citing_paper) AS c\n",
    "WITH COLLECT(cited_paper.paper_id)[1..100] AS most_cited_papers// UNWIND most_cited_papers AS most_cited_paper\n",
    "\n",
    "MATCH (p1:Paper)<-[:Wrote]-(a:Author)-[:Wrote]->(p2:Paper)\n",
    "WHERE p1 <> p2\n",
    "AND p1.paper_id IN most_cited_papers\n",
    "AND p2.paper_id IN most_cited_papers\n",
    "RETURN a\n",
    "LIMIT 100\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "    print(\"Graph Algorithms\")\n",
    "    print(\"Article Rank\")\n",
    "    execute(\"\"\"\n",
    "CALL gds.graph.drop('part_d_1', FALSE);\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "    execute(\"\"\"\n",
    "CALL gds.graph.project(\n",
    "  'part_d_1',\n",
    "  'Paper',\n",
    "  ['Cites', 'PublishedIn']\n",
    ");\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "    execute(\"\"\"\n",
    "CALL gds.articleRank.stream('part_d_1')\n",
    "YIELD nodeId, score\n",
    "RETURN gds.util.asNode(nodeId).title AS name, score\n",
    "ORDER BY score DESC, name ASC;\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"Node Similarity\")\n",
    "    execute(\"\"\"\n",
    "CALL gds.graph.drop('part_d_2', FALSE);\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "    execute(\"\"\"\n",
    "CALL gds.graph.project(\n",
    "  'part_d_2',\n",
    "  ['Author', 'Paper'],\n",
    "  ['Wrote', 'IsCorrespondingAuthor']\n",
    ");\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "    execute(\"\"\"\n",
    "CALL gds.nodeSimilarity.stream('part_d_2')\n",
    "YIELD node1, node2, similarity\n",
    "RETURN gds.util.asNode(node1).name AS Author_1, gds.util.asNode(node2).name AS Author_2, similarity\n",
    "ORDER BY similarity DESC, Author_1, Author_2\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "    print(\"Evolving the graph examples\")\n",
    "    execute(\"\"\"\n",
    "// Adding an affiliation\n",
    "MATCH (a:Author {author_id: '2174735571'})\n",
    "CREATE (a)-[:Affiliated]->(:University {name: 'UPC'});\n",
    "    \"\"\")\n",
    "\n",
    "    execute(\"\"\"\n",
    "// Adding a new review\n",
    "MATCH (a:Author {author_id: '2174735571'}) WITH a\n",
    "MATCH (p:Paper {paper_id: '1da6ce9007a17c60697ca563419d7cc7949ab639'})\n",
    "CREATE (a)-[:Reviewed {review_text: 'Some comments about xyz...', accepted: TRUE}]->(p);\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e63c77-640a-4269-9f28-b3480d73da22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5ec2e681-a429-4361-a899-1475fe8362eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create example DataFrame\n",
    "peopleDF = spark.createDataFrame(\n",
    "    [\n",
    "        {\"name\": \"John\", \"surname\": \"Doe\", \"age\": 42},\n",
    "        {\"name\": \"Jane\", \"surname\": \"Doe\", \"age\": 40},\n",
    "    ]\n",
    ")\n",
    "\n",
    "(\n",
    "    peopleDF.write.format(\"org.neo4j.spark.DataSource\")\n",
    "    .mode(\"Append\")\n",
    "    .option(\"labels\", \":Person\")\n",
    "    .option(\"url\", NEO4J_URL)\n",
    "    .save()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4acc92c1-be4e-4f71-8c31-3684002ad7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.41 ms, sys: 5.44 ms, total: 12.9 ms\n",
      "Wall time: 2.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create example DataFrame\n",
    "relDF = spark.createDataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"name\": \"John\",\n",
    "            \"surname\": \"Doe\",\n",
    "            \"customerID\": 1,\n",
    "            \"product\": \"Product 1\",\n",
    "            \"quantity\": 200,\n",
    "            \"order\": \"ABC100\",\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Jane\",\n",
    "            \"surname\": \"Doe\",\n",
    "            \"customerID\": 2,\n",
    "            \"product\": \"Product 2\",\n",
    "            \"quantity\": 100,\n",
    "            \"order\": \"ABC200\",\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "(\n",
    "relDF.write\n",
    "  # // Create new relationships\n",
    "  .mode('Append')\n",
    "  .format(\"org.neo4j.spark.DataSource\")\n",
    "  # // Assign a type to the relationships\n",
    "  .option(\"relationship\", \"BOUGHT\")\n",
    "  # // Use `keys` strategy\n",
    "  .option(\"relationship.save.strategy\", \"keys\")\n",
    "  # // Overwrite source nodes and assign them a label\n",
    "  .option(\"relationship.source.save.mode\", \"Overwrite\")\n",
    "  .option(\"relationship.source.labels\", \":Customer\")\n",
    "  # // Map the DataFrame columns to node properties\n",
    "  .option(\"relationship.source.node.properties\", \"name,surname,customerID:id\")\n",
    "  # // Node keys are mandatory for overwrite save mode\n",
    "  .option(\"relationship.source.node.keys\", \"customerID:id\")\n",
    "  # // Overwrite target nodes and assign them a label\n",
    "  .option(\"relationship.target.save.mode\", \"Overwrite\")\n",
    "  .option(\"relationship.target.labels\", \":Product\")\n",
    "  # // Map the DataFrame columns to node properties\n",
    "  .option(\"relationship.target.node.properties\", \"product:name\")\n",
    "  # // Node keys are mandatory for overwrite save mode\n",
    "  .option(\"relationship.target.node.keys\", \"product:blah\")\n",
    "  # // Map the DataFrame columns to relationship properties\n",
    "  .option(\"relationship.properties\", \"quantity,order\")\n",
    "  .option(\"url\", NEO4J_URL)\n",
    "  .save()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0832874b-7897-400d-bf48-6091985d1d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    relDF.write\n",
    "    # Create new relationships\n",
    "    .mode(\"Overwrite\")\n",
    "    .format(\"org.neo4j.spark.DataSource\")\n",
    "    # Assign a type to the relationships\n",
    "    .option(\"relationship\", \"BOUGHT\")\n",
    "    # Use `keys` strategy\n",
    "    .option(\"relationship.save.strategy\", \"keys\")\n",
    "    # Don't create new nodes\n",
    "    .option(\"relationship.source.save.mode\", \"Overwrite\")\n",
    "    # Create source nodes and assign them a label\n",
    "    .option(\"relationship.source.save.mode\", \"Append\")\n",
    "    .option(\"relationship.source.labels\", \":Customer\")\n",
    "    # Map the DataFrame columns to node properties\n",
    "    .option(\"relationship.source.node.properties\", \"name,surname:suuuurname,customerID:<id>\")\n",
    "    # Create target nodes and assign them a label\n",
    "    .option(\"relationship.target.save.mode\", \"Append\")\n",
    "    .option(\"relationship.target.labels\", \":Product\")\n",
    "    # Map the DataFrame columns to node properties\n",
    "    .option(\"relationship.target.node.properties\", \"product:name\")\n",
    "    # Map the DataFrame columns to relationship properties\n",
    "    .option(\"relationship.properties\", \"quantity,order\")\n",
    "    .option(\"url\", NEO4J_URL)\n",
    "    .save()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
